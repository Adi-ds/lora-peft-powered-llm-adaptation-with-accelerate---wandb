{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":6658430,"sourceType":"datasetVersion","datasetId":3842486}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #9467bd;\n    padding: 20px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #ff7f00;\n}\n\nh2 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #de9ed6;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #800080;\n}\n\nh3 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #756bb1;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #393b79;\n}\n\nbody, p {\n    font-family: ariel;\n    font-size: 18px;\n    color: black;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: purple;\n}\n\n</style>\n\"\"\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-12-24T19:01:21.111275Z","iopub.execute_input":"2023-12-24T19:01:21.111591Z","iopub.status.idle":"2023-12-24T19:01:21.126458Z","shell.execute_reply.started":"2023-12-24T19:01:21.111540Z","shell.execute_reply":"2023-12-24T19:01:21.125595Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #9467bd;\n    padding: 20px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #ff7f00;\n}\n\nh2 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #de9ed6;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #800080;\n}\n\nh3 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #756bb1;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #393b79;\n}\n\nbody, p {\n    font-family: ariel;\n    font-size: 18px;\n    color: black;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: purple;\n}\n\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"Now, let's serve the food which we already [cooked](https://www.kaggle.com/code/adityadawn/training-llm-using-accelerate-and-w-b).","metadata":{}},{"cell_type":"markdown","source":"## Installing required Libraries to load a trained LLM","metadata":{}},{"cell_type":"code","source":"!pip install peft==0.4.0\n!pip install torch==2.0.0\n!pip install datasets==2.1.0\n!pip install bitsandbytes==0.41.1\n!pip install transformers==4.30.2\n!pip install transformers[torch]\n!pip install transformers[sentencepiece]\n\n!pip install huggingface_hub==0.16.4\n\nfrom IPython.display import clear_output\nclear_output()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-24T19:01:21.131239Z","iopub.execute_input":"2023-12-24T19:01:21.131597Z","iopub.status.idle":"2023-12-24T19:03:11.833672Z","shell.execute_reply.started":"2023-12-24T19:01:21.131571Z","shell.execute_reply":"2023-12-24T19:03:11.832567Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Importing Required Libraries and Modules","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel\nfrom peft import LoraConfig\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\n\nimport gc\nimport pandas as pd\nfrom tqdm import tqdm\nimport huggingface_hub\nfrom string import Template\nfrom kaggle_secrets import UserSecretsClient","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:11.835868Z","iopub.execute_input":"2023-12-24T19:03:11.836189Z","iopub.status.idle":"2023-12-24T19:03:25.097613Z","shell.execute_reply.started":"2023-12-24T19:03:11.836158Z","shell.execute_reply":"2023-12-24T19:03:25.096714Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Using Secret Ingredients ðŸ¤«","metadata":{}},{"cell_type":"code","source":"# Fetch Secrets\nuser_secrets=UserSecretsClient()\n\n# Fetch secret keys\nmy_secret_hf_api_key=user_secrets.get_secret(\"hf_model_read\")\n\n# Use the secret keys to login\nhuggingface_hub.login(token=my_secret_hf_api_key, add_to_git_credential=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.098829Z","iopub.execute_input":"2023-12-24T19:03:25.099444Z","iopub.status.idle":"2023-12-24T19:03:25.709735Z","shell.execute_reply.started":"2023-12-24T19:03:25.099412Z","shell.execute_reply":"2023-12-24T19:03:25.708759Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"markdown","source":"### Unlocking BNB's Hidden Potential: Mastering Quantization Configuration","metadata":{}},{"cell_type":"markdown","source":"**Configuring the BNB Spice Blend**: Imagine crafting the perfect curry. You need the right mix of fiery chilies, aromatic spices, and creamy coconut milk. This function plays the chef, carefully preparing the \"Bits & Bytes\" configuration, ensuring your PEFT model has the right balance of precision and efficiency â€“ a recipe for AI success!","metadata":{}},{"cell_type":"code","source":"def config_bnb(Config_BNB):\n    \"\"\"Creates a BitsAndBytesConfig object with settings from a configuration object.\n\n   Args:\n   - Config_BNB (object): A configuration object containing BNB-related settings.\n\n   Returns:\n   - BitsAndBytesConfig: A configured BitsAndBytesConfig object.\n\n   Raises:\n   - Exception: If any errors occur during configuration.\n   \"\"\"\n    try:\n        return BitsAndBytesConfig(\n                    load_in_4bit=Config_BNB.LOAD_IN_4BIT,\n                    bnb_4bit_quant_type=Config_BNB.BNB_4BIT_QUANT_TYPE,\n                    bnb_4bit_compute_dtype=Config_BNB.BNB_4BIT_COMPUTE_DTYPE,\n                    bnb_4bit_use_double_quant=Config_BNB.BNB_4BIT_USE_DOUBLE_QUANT,\n                    load_in_8bit_fp32_cpu_offload=Config_BNB.LOAD_IN_8BIT_FP32_CPU_OFFLOAD\n                )\n    except Exception as e:\n        raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.712517Z","iopub.execute_input":"2023-12-24T19:03:25.713475Z","iopub.status.idle":"2023-12-24T19:03:25.718770Z","shell.execute_reply.started":"2023-12-24T19:03:25.713436Z","shell.execute_reply":"2023-12-24T19:03:25.717853Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Charting the Course: Unveiling the Recipe Behind a PEFT Masterpiece","metadata":{}},{"cell_type":"markdown","source":"**Unveiling the Model's Recipe Book**: Every great dish starts with a plan. This function serves as your trusty sous-chef, pulling out the precise \"model configuration\" â€“ the secret ingredients and instructions that tell your PEFT model what to do. With this knowledge, you can adjust flavors and cook up the perfect AI dish.","metadata":{}},{"cell_type":"code","source":"def get_model_config(peft_model_id: str):\n    \"\"\"Retrieves the model configuration for a given PEFT model ID.\n\n    Args:\n    - peft_model_id (str): The ID of the PEFT model to load.\n\n    Returns:\n    - LoraConfig: The loaded model configuration.\n\n    Raises:\n    - ValueError: If the model configuration cannot be loaded from the pretrained model.\n    \"\"\"\n    return LoraConfig.from_pretrained(peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.720191Z","iopub.execute_input":"2023-12-24T19:03:25.720548Z","iopub.status.idle":"2023-12-24T19:03:25.729224Z","shell.execute_reply.started":"2023-12-24T19:03:25.720496Z","shell.execute_reply":"2023-12-24T19:03:25.728377Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### From Blueprint to Brushstrokes: Assembling the Model from Configuration Blocks","metadata":{}},{"cell_type":"markdown","source":"**Summoning the AI Master Chef**: Picture the sizzle and steam as you assemble a magnificent paella. This function takes the model configuration and brings it to life, conjuring a complete PEFT model ready to work its magic. Imagine the possibilities â€“ creating your own AI masterpiece!","metadata":{}},{"cell_type":"code","source":"def get_model(peft_model_id: str, model_config):\n    \"\"\"Loads a PEFT model with quantization configuration.\n\n    Args:\n    - peft_model_id (str): The ID of the PEFT model to load.\n    - model_config (LoraConfig): The configuration of the model.\n\n    Returns:\n    - PeftModel: The loaded PEFT model.\n    \"\"\"\n    model = AutoModelForCausalLM.from_pretrained(\n                model_config.base_model_name_or_path,\n                return_dict=True,\n                quantization_config=config_bnb(Config_BNB),\n                trust_remote_code=True,\n                load_in_4bit=True,\n                device_map={\"\":0},\n            )\n    model = PeftModel.from_pretrained(\n                model, \n                peft_model_id, \n                device_map={\"\":0}\n            )\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.720191Z","iopub.execute_input":"2023-12-24T19:03:25.720548Z","iopub.status.idle":"2023-12-24T19:03:25.729224Z","shell.execute_reply.started":"2023-12-24T19:03:25.720496Z","shell.execute_reply":"2023-12-24T19:03:25.728377Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Unlocking the Code of Communication: Translating Human Words into AI Language","metadata":{}},{"cell_type":"markdown","source":"**Chopping the Words for AI Appetizers**: Communication is key in any kitchen, and AI needs its own language. This function acts as the \"word chopper,\" transforming human speech into bite-sized \"tokens\" that the PEFT model can understand. It's the essential garnish that lets your AI feast on your linguistic offerings.","metadata":{}},{"cell_type":"code","source":"def get_tokenizer(model_config):\n    \"\"\"\n    Loads the tokenizer associated with a model configuration.\n\n    Args:\n    - model_config (LoraConfig): The configuration of the model.\n\n    Returns:\n    - AutoTokenizer: The loaded tokenizer.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_config.base_model_name_or_path)\n    tokenizer.pad_token = tokenizer.eos_token\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.720191Z","iopub.execute_input":"2023-12-24T19:03:25.720548Z","iopub.status.idle":"2023-12-24T19:03:25.729224Z","shell.execute_reply.started":"2023-12-24T19:03:25.720496Z","shell.execute_reply":"2023-12-24T19:03:25.728377Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### From Chaos to Clarity: Crafting a Brain-Teasing Prompt for AI","metadata":{}},{"cell_type":"markdown","source":"**Setting the AI Dinner Table**: Before the main course, you arrange the plates and utensils. This function preps the test \"prompt,\" meticulously setting the stage for the PEFT model to show off its skills. It's the elegant presentation that makes your AI's performance truly shine.","metadata":{}},{"cell_type":"code","source":"def prepare_final_prompt_test(\n        row_data: pd.core.series.Series,\n        instruction: str='As your response to the prompt, select the most appropriate option among A, B, C, D, and E.'\n    ):\n    \"\"\"\n    Prepares a final prompt for testing a model's ability to select the correct answer from multiple choices.\n\n    Args:\n    - row_data (pd.core.series.Series): A pandas Series containing the prompt, options (A, B, C, D, E), and answer.\n    - instruction (str, optional): An instruction to present to the model before the prompt. Defaults to 'As your response to the prompt, select the most appropriate option among A, B, C, D, and E.'.\n\n    Returns:\n    - str: The formatted prompt ready for model input.\n\n    Raises:\n    - Exception: If any errors occur during template substitution.\n    \"\"\"\n        try:\n            template = Template(\n                            '### Instruction: \\n \\\n                             $instruction \\n\\n   \\\n                             ### Prompt \\n       \\\n                             $prompt \\n\\n        \\\n                             A) $a \\n            \\\n                             B) $b \\n            \\\n                             C) $c \\n            \\\n                             D) $d \\n            \\\n                             E) $e \\n\\n          \\\n                             ### Answer'\n                        )\n            final_prompt = template.substitute(\n                                instruction = instruction,\n                                prompt = row_data['prompt'],\n                                a = row_data['A'],\n                                b = row_data['B'],\n                                c = row_data['C'],\n                                d = row_data['D'],\n                                e = row_data['E']\n                            )\n            \n            del row_data\n            gc.collect()\n            \n            return final_prompt\n        except Exception as e:\n            raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.730316Z","iopub.execute_input":"2023-12-24T19:03:25.730699Z","iopub.status.idle":"2023-12-24T19:03:25.739320Z","shell.execute_reply.started":"2023-12-24T19:03:25.730664Z","shell.execute_reply":"2023-12-24T19:03:25.738307Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Beyond Multiple Choice: Uncovering the AI's Choice with Confidence","metadata":{}},{"cell_type":"markdown","source":"**Extracting the AI's Inner Critic**: After a delicious meal, you ask for feedback. This function acts as the AI's \"inner critic,\" peeking into its neural network and extracting the chosen answer, along with its confidence level. It's the secret sauce that lets you understand what went into the AI's culinary creation.","metadata":{}},{"cell_type":"code","source":"def get_ans(\n        text,\n        tokenizer,\n        model\n    ):\n    \"\"\"\n    Predicts the answer to a prompt with multiple choices using a PEFT model.\n\n    Args:\n    - text (str): The prompt text, including multiple choices.\n    - tokenizer (AutoTokenizer): The tokenizer to process the text.\n    - model (PeftModel): The PEFT model to generate predictions.\n\n    Returns:\n    - str: The predicted answer (A, B, C, D, or E).\n\n    Raises:\n    - ValueError: If the model's prediction is invalid (e.g., outside the expected answer choices).\n    \"\"\"\n    inputs = tokenizer(\n                text, \n                return_tensors='pt'\n            )\n    logits = model(\n                input_ids=inputs['input_ids'].cuda(), \n                attention_mask=inputs['attention_mask'].cuda()\n            ).logits[0, -1]\n    \n    options_list = [\n                        (logits[tokenizer('A').input_ids[-1]], 'A'), \n                        (logits[tokenizer('B').input_ids[-1]], 'B'), \n                        (logits[tokenizer('C').input_ids[-1]], 'C'), \n                        (logits[tokenizer('D').input_ids[-1]], 'D'), \n                        (logits[tokenizer('E').input_ids[-1]], 'E')\n                    ] \n    options_list = sorted(options_list, reverse=True)\n    ans = options_list[1]\n        \n    return ans","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.740705Z","iopub.execute_input":"2023-12-24T19:03:25.741116Z","iopub.status.idle":"2023-12-24T19:03:25.751916Z","shell.execute_reply.started":"2023-12-24T19:03:25.741087Z","shell.execute_reply":"2023-12-24T19:03:25.751038Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### From Prompt to Prophecy: Unveiling the Future with a PEFT's Predictions","metadata":{}},{"cell_type":"markdown","source":"**Predicting a Banquet of Solutions**: Imagine having a crystal ball that shows every possible dessert course. This function taps into the PEFT model's predictive power, generating an entire banquet of \"predictions\" for a whole dataset. It's like seeing the future of AI-powered solutions unfold before your eyes.","metadata":{}},{"cell_type":"code","source":"def get_predictions(\n        test_dataset,\n        tokenizer,\n        model\n    ):\n    \"\"\"\n    Generates predictions for a test dataset using a PEFT model.\n\n    Args:\n    - test_dataset (Iterable[dict]): An iterable of dictionaries, each containing a 'final_prompt' key.\n    - tokenizer (AutoTokenizer): The tokenizer to process the text.\n    - model (PeftModel): The PEFT model to generate predictions.\n\n    Returns:\n    - Tuple[List[int], List[str]]: A tuple containing two lists:\n        - ids (List[int]): A list of IDs corresponding to each data point in the test dataset.\n        - predictions (List[str]): A list of predicted answers (A, B, C, D, or E) for each prompt.\n\n    Raises:\n        ValueError: If any errors occur during prediction generation.\n    \"\"\"\n    id = []\n    prediction = []\n\n    for idx, text in tqdm(\n                        enumerate(test_dataset),\n                        total = len(test_dataset),\n                        desc = 'Obtainineg Predictions'\n                    ):\n        prediction.append(\n                            get_ans(\n                                text['final_prompt'],\n                                tokenizer,\n                                model\n                            )[1]\n                         )\n        id.append(idx)\n    \n    return id, prediction","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.753232Z","iopub.execute_input":"2023-12-24T19:03:25.753579Z","iopub.status.idle":"2023-12-24T19:03:25.765207Z","shell.execute_reply.started":"2023-12-24T19:03:25.753546Z","shell.execute_reply":"2023-12-24T19:03:25.764261Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### From Training Ground to Battlefield: Launching a PEFT's Inference Mission","metadata":{}},{"cell_type":"markdown","source":"**The Grand AI Buffet Opens**: It's finally time to indulge! This function throws open the doors, unleashing the PEFT model on real-world data. It's the grand reveal, the moment you witness the culmination of all your efforts â€“ your AI creation, ready to serve up a feast of insights and solutions.","metadata":{}},{"cell_type":"code","source":"def inference(\n        peft_model_id: str,\n        test_csv_path: str\n    ):\n    \"\"\"Performs inference using a PEFT model on a test dataset.\n\n    Args:\n    - peft_model_id (str): The ID of the PEFT model to use.\n    - test_csv_path (str): The path to the CSV file containing test data.\n\n    Returns:\n    - Tuple[List[int], List[str]]: A tuple containing:\n        - ids (List[int]): A list of IDs corresponding to each data point in the test dataset.\n        - predictions (List[str]): A list of predicted answers for each prompt.\n    \"\"\"\n\n    # Obtaining trained model and tokenizer\n    model_config = get_model_config(peft_model_id)\n    model = get_model(peft_model_id, model_config)\n    tokenizer = get_tokenizer(model_config)\n    \n    # Prepraring Test dataset\n    test_df = pd.read_csv(TEST_CSV_PATH)\n    test_df['final_prompt'] = test_df.apply(prepare_final_prompt_test, axis=1)\n    test_ds = Dataset.from_pandas(pd.DataFrame(test_df['final_prompt']))\n    \n    ids, predictions = get_predictions(\n                            test_ds,\n                            tokenizer,\n                            model\n                        )\n    \n    return ids, predictions","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.766255Z","iopub.execute_input":"2023-12-24T19:03:25.766520Z","iopub.status.idle":"2023-12-24T19:03:25.775809Z","shell.execute_reply.started":"2023-12-24T19:03:25.766485Z","shell.execute_reply":"2023-12-24T19:03:25.774822Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Finally, let's prepare the table","metadata":{}},{"cell_type":"code","source":"PEFT_MODEL_ID = '/kaggle/input/kaggle-scienc-llm-model/kaggle/working/Kaggle-Science-LLM'\nTEST_CSV_PATH = '/kaggle/input/kaggle-llm-science-exam/test.csv'\n\n@dataclass(frozen=True)\nclass Config_BNB:\n    LOAD_BIT: int=4\n    LOAD_IN_4BIT: bool=True\n    BNB_4BIT_QUANT_TYPE: str=\"nf4\"\n    BNB_4BIT_USE_DOUBLE_QUANT: bool=True\n    BNB_4BIT_COMPUTE_DTYPE=torch.bfloat16\n    LOAD_IN_8BIT_FP32_CPU_OFFLOAD: bool=True","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.778403Z","iopub.execute_input":"2023-12-24T19:03:25.778770Z","iopub.status.idle":"2023-12-24T19:03:25.789040Z","shell.execute_reply.started":"2023-12-24T19:03:25.778734Z","shell.execute_reply":"2023-12-24T19:03:25.788130Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Let's serve the food","metadata":{}},{"cell_type":"code","source":"id, prediction = inference(PEFT_MODEL_ID, TEST_CSV_PATH)\n\nprediction_df = pd.DataFrame(\n                    data={\n                        'id':id,\n                        'prediction':prediction\n                    }\n                )","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:03:25.790565Z","iopub.execute_input":"2023-12-24T19:03:25.790929Z","iopub.status.idle":"2023-12-24T19:20:01.594226Z","shell.execute_reply.started":"2023-12-24T19:03:25.790895Z","shell.execute_reply":"2023-12-24T19:20:01.593026Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cf1af58da2f4216a83b3854963998b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f284034c0f4965b93b8d394fde9286"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7feec3c2f9e42e5899d93ee17a2eec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f664c1bfa0244ccb88c2ca41902005cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ff62fe522c4f9fb4b552c3b0ec55f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a269108b6c8e4a4c9a9c98b7b7998340"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac7ec5289a54def99713eca13f9cf19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a464f173fdd4552902a0b9ed9e85d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"852caf947e1f449dbac15fcd44893769"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"177312c5795c42ce913b91a69d2b6b45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fa8b2da73fd4b9f98b9288e181c881e"}},"metadata":{}},{"name":"stderr","text":"Obtainineg Predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [03:41<00:00,  1.11s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## And ... Bon AppÃ©tit!","metadata":{}},{"cell_type":"code","source":"prediction_df","metadata":{"execution":{"iopub.status.busy":"2023-12-24T19:21:46.660533Z","iopub.execute_input":"2023-12-24T19:21:46.661464Z","iopub.status.idle":"2023-12-24T19:21:46.678191Z","shell.execute_reply.started":"2023-12-24T19:21:46.661419Z","shell.execute_reply":"2023-12-24T19:21:46.677200Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"      id prediction\n0      0          D\n1      1          C\n2      2          D\n3      3          D\n4      4          D\n..   ...        ...\n195  195          B\n196  196          D\n197  197          D\n198  198          B\n199  199          E\n\n[200 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>195</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>196</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>197</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>198</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>199</td>\n      <td>E</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Special Mentions:\n1. [Falcon-7b LLM (QLORA)W&B](https://www.kaggle.com/code/mayank00rastogi/falcon-7b-llm-qlora-w-b)\n2. [Falcon-7bðŸš€(InferencingðŸ¤–)](https://www.kaggle.com/code/mayank00rastogi/falcon-7b-inferencing)","metadata":{}},{"cell_type":"markdown","source":"#### Endnote: This has been a wonderful journey. I had a great time learning all of these things! The main challenges were to make this work and also explain each step. In fact, at the end, I still think, 'I could have done that differently!', 'Maybe the writings are too illustrative', etc. So please, if you think I have made any mistakes or could have approached a specific step in the entire project differently, do let me know in the comments.","metadata":{}},{"cell_type":"markdown","source":"# Please upvote if you like!","metadata":{}}]}