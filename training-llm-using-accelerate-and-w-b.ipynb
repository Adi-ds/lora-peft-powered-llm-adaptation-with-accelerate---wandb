{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #9467bd;\n    padding: 20px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #ff7f00;\n}\n\nh2 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #de9ed6;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #800080;\n}\n\nh3 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #756bb1;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #393b79;\n}\n\nbody, p {\n    font-family: ariel;\n    font-size: 18px;\n    color: black;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: purple;\n}\n\n</style>\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2023-12-23T15:53:20.940114Z","iopub.execute_input":"2023-12-23T15:53:20.940629Z","iopub.status.idle":"2023-12-23T15:53:20.954601Z","shell.execute_reply.started":"2023-12-23T15:53:20.940603Z","shell.execute_reply":"2023-12-23T15:53:20.953822Z"},"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #9467bd;\n    padding: 20px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #ff7f00;\n}\n\nh2 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #de9ed6;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #800080;\n}\n\nh3 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    background-color: #756bb1;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #393b79;\n}\n\nbody, p {\n    font-family: ariel;\n    font-size: 18px;\n    color: black;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: purple;\n}\n\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# A Culinary Tour of Language Model Training","metadata":{}},{"cell_type":"markdown","source":"Have you ever wondered about the intricate process behind training a large language model? It's not unlike preparing a gourmet meal! Each step requires careful attention and precise execution, from gathering the finest ingredients to mastering cooking techniques. A short display of the process is provided in the following flowchart!","metadata":{}},{"cell_type":"markdown","source":"## Flowchart of the training process","metadata":{}},{"cell_type":"code","source":"import pydot\nfrom IPython.display import SVG\n\ntraining_flow = pydot.Dot(graph_type='digraph')\n\ndata_folder_node = pydot.Node('Data\\nFolder')\ndata_folder_node.set_shape('folder')\ntraining_flow.add_node(data_folder_node)\n\ntraining_data_csv_node = pydot.Node('Training\\ndata')\ntraining_data_csv_node.set_shape('file')\ntraining_flow.add_node(training_data_csv_node)\n\nvalidation_data_csv_node = pydot.Node('Validation\\ndata')\nvalidation_data_csv_node.set_shape('file')\ntraining_flow.add_node(validation_data_csv_node)\n\ndata_folder_train_file_edge = pydot.Edge(data_folder_node, training_data_csv_node)\ntraining_flow.add_edge(data_folder_train_file_edge)\n\ndata_folder_valid_file_edge = pydot.Edge(data_folder_node, validation_data_csv_node)\ntraining_flow.add_edge(data_folder_valid_file_edge)\n\ndataset_conversion_node = pydot.Node('Creating\\nDataset')\ndataset_conversion_node.set_shape('rect')\ntraining_flow.add_node(dataset_conversion_node)\n\ntrain_csv_dataset_edge = pydot.Edge(training_data_csv_node, dataset_conversion_node)\ntraining_flow.add_edge(train_csv_dataset_edge)\n\nvalid_csv_dataset_edge = pydot.Edge(validation_data_csv_node, dataset_conversion_node)\ntraining_flow.add_edge(valid_csv_dataset_edge)\n\ntrain_dataset_maker_node = pydot.Node('Train\\nDataset')\ntrain_dataset_maker_node.set_shape('rect')\ntraining_flow.add_node(train_dataset_maker_node)\n\nvalid_dataset_maker_node = pydot.Node('Valid\\nDataset')\nvalid_dataset_maker_node.set_shape('rect')\ntraining_flow.add_node(valid_dataset_maker_node)\n\ntrain_dataset_edge = pydot.Edge(dataset_conversion_node, train_dataset_maker_node)\ntraining_flow.add_edge(train_dataset_edge)\n\nvalid_dataset_edge = pydot.Edge(dataset_conversion_node, valid_dataset_maker_node)\ntraining_flow.add_edge(valid_dataset_edge)\n\ntrain_dataloader_node = pydot.Node('Train\\nDataloader')\ntrain_dataloader_node.set_shape('rect')\ntraining_flow.add_node(train_dataloader_node)\n\ntrain_data_dataloader_edge = pydot.Edge(train_dataset_maker_node, train_dataloader_node)\ntraining_flow.add_edge(train_data_dataloader_edge)\n\nlr_scheduler_node = pydot.Node('LR\\nScheduler')\nlr_scheduler_node.set_shape('rect')\ntraining_flow.add_node(lr_scheduler_node)\n\ntrain_dataloader_to_lr_edge = pydot.Edge(train_dataloader_node, lr_scheduler_node)\ntraining_flow.add_edge(train_dataloader_to_lr_edge)\n\ncustom_optimizer_node = pydot.Node('Custom\\nOptimizer')\ncustom_optimizer_node.set_shape('rect')\ntraining_flow.add_node(custom_optimizer_node)\n\nfinal_optimizer_node = pydot.Node('Optimizers')\nfinal_optimizer_node.set_shape('rect')\ntraining_flow.add_node(final_optimizer_node)\n\nlr_to_final_optimizer_edge = pydot.Edge(lr_scheduler_node, final_optimizer_node)\ntraining_flow.add_edge(lr_to_final_optimizer_edge)\n\ncustom_optimizer_to_final_optimizer_edge = pydot.Edge(custom_optimizer_node, final_optimizer_node)\ntraining_flow.add_edge(custom_optimizer_to_final_optimizer_edge)\n\ntrainer_node = pydot.Node('Trainer')\ntrainer_node.set_shape('rect')\ntraining_flow.add_node(trainer_node)\n\noptimizer_to_trainer_edge = pydot.Edge(final_optimizer_node, trainer_node)\ntraining_flow.add_edge(optimizer_to_trainer_edge)\n\ntrain_dataset_to_trainer_edge = pydot.Edge(train_dataset_maker_node, trainer_node)\ntraining_flow.add_edge(train_dataset_to_trainer_edge)\n\nvalid_dataset_to_trainer_edge = pydot.Edge(valid_dataset_maker_node, trainer_node)\ntraining_flow.add_edge(valid_dataset_to_trainer_edge)\n\ndata_collator_node = pydot.Node('Data\\nCollator')\ndata_collator_node.set_shape('rect')\ntraining_flow.add_node(data_collator_node)\n\ndata_collator_to_train_dataloader_edge = pydot.Edge(data_collator_node, train_dataloader_node)\ntraining_flow.add_edge(data_collator_to_train_dataloader_edge)\n\ndata_collator_to_trainer_edge = pydot.Edge(data_collator_node, trainer_node)\ntraining_flow.add_edge(data_collator_to_trainer_edge)\n\ntokenizer_node = pydot.Node('Tokenizer')\ntokenizer_node.set_shape('rect')\ntraining_flow.add_node(tokenizer_node)\n\nmodel_node = pydot.Node('Model')\nmodel_node.set_shape('rect')\ntraining_flow.add_node(model_node)\n\ntokenizer_trainer_edge = pydot.Edge(tokenizer_node, trainer_node)\ntraining_flow.add_edge(tokenizer_trainer_edge)\n\ntokenizer_model_edge = pydot.Edge(tokenizer_node, model_node)\ntraining_flow.add_edge(tokenizer_model_edge)\n\nmodel_trainer_edge = pydot.Edge(model_node, trainer_node)\ntraining_flow.add_edge(model_trainer_edge)\n\nlora_node = pydot.Node('LORA\\nSetting')\nlora_node.set_shape('rect')\ntraining_flow.add_node(lora_node)\n\nlora_to_model_edge = pydot.Edge(lora_node, model_node)\ntraining_flow.add_edge(lora_to_model_edge)\n\nbnb_node = pydot.Node('BNB\\nSetting')\nbnb_node.set_shape('rect')\ntraining_flow.add_node(bnb_node)\n\nbnb_to_model_edge = pydot.Edge(bnb_node, model_node)\ntraining_flow.add_edge(bnb_to_model_edge)\n\ntrain_node = pydot.Node('train')\ntrain_node.set_shape('rect')\ntraining_flow.add_node(train_node)\n\ntrainer_train_edge = pydot.Edge(trainer_node, train_node)\ntraining_flow.add_edge(trainer_train_edge)\n\ntraining_flow.write_svg('flow.svg')\nSVG('flow.svg')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-23T19:50:15.239637Z","iopub.execute_input":"2023-12-23T19:50:15.240171Z","iopub.status.idle":"2023-12-23T19:50:15.317014Z","shell.execute_reply.started":"2023-12-23T19:50:15.240130Z","shell.execute_reply":"2023-12-23T19:50:15.315414Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.SVG object>","image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"665pt\" height=\"659pt\" viewBox=\"0.00 0.00 665.25 659.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 655)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-655 661.25,-655 661.25,4 -4,4\"/>\n<!-- Data\\nFolder -->\n<g id=\"node1\" class=\"node\">\n<title>Data\\nFolder</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"118.88,-651 115.88,-655 94.88,-655 91.88,-651 57.12,-651 57.12,-608.5 118.88,-608.5 118.88,-651\"/>\n<text text-anchor=\"middle\" x=\"88\" y=\"-633.7\" font-family=\"Times,serif\" font-size=\"14.00\">Data</text>\n<text text-anchor=\"middle\" x=\"88\" y=\"-616.45\" font-family=\"Times,serif\" font-size=\"14.00\">Folder</text>\n</g>\n<!-- Training\\ndata -->\n<g id=\"node2\" class=\"node\">\n<title>Training\\ndata</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"76,-572.5 0,-572.5 0,-530 76,-530 76,-572.5\"/>\n<text text-anchor=\"middle\" x=\"38\" y=\"-555.2\" font-family=\"Times,serif\" font-size=\"14.00\">Training</text>\n<text text-anchor=\"middle\" x=\"38\" y=\"-537.95\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n</g>\n<!-- Data\\nFolder&#45;&gt;Training\\ndata -->\n<g id=\"edge1\" class=\"edge\">\n<title>Data\\nFolder-&gt;Training\\ndata</title>\n<path fill=\"none\" stroke=\"black\" d=\"M74.6,-608.25C69.33,-600.19 63.18,-590.78 57.44,-582\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"59.84,-580.26 51.44,-573.81 53.98,-584.09 59.84,-580.26\"/>\n</g>\n<!-- Validation\\ndata -->\n<g id=\"node3\" class=\"node\">\n<title>Validation\\ndata</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"181.62,-572.5 94.38,-572.5 94.38,-530 181.62,-530 181.62,-572.5\"/>\n<text text-anchor=\"middle\" x=\"138\" y=\"-555.2\" font-family=\"Times,serif\" font-size=\"14.00\">Validation</text>\n<text text-anchor=\"middle\" x=\"138\" y=\"-537.95\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n</g>\n<!-- Data\\nFolder&#45;&gt;Validation\\ndata -->\n<g id=\"edge2\" class=\"edge\">\n<title>Data\\nFolder-&gt;Validation\\ndata</title>\n<path fill=\"none\" stroke=\"black\" d=\"M101.4,-608.25C106.67,-600.19 112.82,-590.78 118.56,-582\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"122.02,-584.09 124.56,-573.81 116.16,-580.26 122.02,-584.09\"/>\n</g>\n<!-- Creating\\nDataset -->\n<g id=\"node4\" class=\"node\">\n<title>Creating\\nDataset</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"127.12,-494 48.88,-494 48.88,-451.5 127.12,-451.5 127.12,-494\"/>\n<text text-anchor=\"middle\" x=\"88\" y=\"-476.7\" font-family=\"Times,serif\" font-size=\"14.00\">Creating</text>\n<text text-anchor=\"middle\" x=\"88\" y=\"-459.45\" font-family=\"Times,serif\" font-size=\"14.00\">Dataset</text>\n</g>\n<!-- Training\\ndata&#45;&gt;Creating\\nDataset -->\n<g id=\"edge3\" class=\"edge\">\n<title>Training\\ndata-&gt;Creating\\nDataset</title>\n<path fill=\"none\" stroke=\"black\" d=\"M51.4,-529.75C56.67,-521.69 62.82,-512.28 68.56,-503.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"72.02,-505.59 74.56,-495.31 66.16,-501.76 72.02,-505.59\"/>\n</g>\n<!-- Validation\\ndata&#45;&gt;Creating\\nDataset -->\n<g id=\"edge4\" class=\"edge\">\n<title>Validation\\ndata-&gt;Creating\\nDataset</title>\n<path fill=\"none\" stroke=\"black\" d=\"M124.6,-529.75C119.33,-521.69 113.18,-512.28 107.44,-503.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"109.84,-501.76 101.44,-495.31 103.98,-505.59 109.84,-501.76\"/>\n</g>\n<!-- Train\\nDataset -->\n<g id=\"node5\" class=\"node\">\n<title>Train\\nDataset</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"155.75,-415.5 84.25,-415.5 84.25,-373 155.75,-373 155.75,-415.5\"/>\n<text text-anchor=\"middle\" x=\"120\" y=\"-398.2\" font-family=\"Times,serif\" font-size=\"14.00\">Train</text>\n<text text-anchor=\"middle\" x=\"120\" y=\"-380.95\" font-family=\"Times,serif\" font-size=\"14.00\">Dataset</text>\n</g>\n<!-- Creating\\nDataset&#45;&gt;Train\\nDataset -->\n<g id=\"edge5\" class=\"edge\">\n<title>Creating\\nDataset-&gt;Train\\nDataset</title>\n<path fill=\"none\" stroke=\"black\" d=\"M96.58,-451.25C99.84,-443.45 103.63,-434.38 107.21,-425.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"110.77,-427.38 111.4,-416.81 104.31,-424.68 110.77,-427.38\"/>\n</g>\n<!-- Valid\\nDataset -->\n<g id=\"node6\" class=\"node\">\n<title>Valid\\nDataset</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"91.75,-337 20.25,-337 20.25,-294.5 91.75,-294.5 91.75,-337\"/>\n<text text-anchor=\"middle\" x=\"56\" y=\"-319.7\" font-family=\"Times,serif\" font-size=\"14.00\">Valid</text>\n<text text-anchor=\"middle\" x=\"56\" y=\"-302.45\" font-family=\"Times,serif\" font-size=\"14.00\">Dataset</text>\n</g>\n<!-- Creating\\nDataset&#45;&gt;Valid\\nDataset -->\n<g id=\"edge6\" class=\"edge\">\n<title>Creating\\nDataset-&gt;Valid\\nDataset</title>\n<path fill=\"none\" stroke=\"black\" d=\"M83.06,-451.22C80.55,-440.58 77.51,-427.36 75,-415.5 70.22,-392.94 65.32,-367.37 61.69,-347.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"64.97,-347.42 59.71,-338.22 58.09,-348.69 64.97,-347.42\"/>\n</g>\n<!-- Train\\nDataloader -->\n<g id=\"node7\" class=\"node\">\n<title>Train\\nDataloader</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"342.38,-337 247.62,-337 247.62,-294.5 342.38,-294.5 342.38,-337\"/>\n<text text-anchor=\"middle\" x=\"295\" y=\"-319.7\" font-family=\"Times,serif\" font-size=\"14.00\">Train</text>\n<text text-anchor=\"middle\" x=\"295\" y=\"-302.45\" font-family=\"Times,serif\" font-size=\"14.00\">Dataloader</text>\n</g>\n<!-- Train\\nDataset&#45;&gt;Train\\nDataloader -->\n<g id=\"edge7\" class=\"edge\">\n<title>Train\\nDataset-&gt;Train\\nDataloader</title>\n<path fill=\"none\" stroke=\"black\" d=\"M155.84,-377.58C179.44,-367.27 210.85,-353.54 237.86,-341.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"238.83,-344.69 246.59,-337.48 236.02,-338.27 238.83,-344.69\"/>\n</g>\n<!-- Trainer -->\n<g id=\"node11\" class=\"node\">\n<title>Trainer</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"366.25,-108 297.75,-108 297.75,-72 366.25,-72 366.25,-108\"/>\n<text text-anchor=\"middle\" x=\"332\" y=\"-85.33\" font-family=\"Times,serif\" font-size=\"14.00\">Trainer</text>\n</g>\n<!-- Train\\nDataset&#45;&gt;Trainer -->\n<g id=\"edge12\" class=\"edge\">\n<title>Train\\nDataset-&gt;Trainer</title>\n<path fill=\"none\" stroke=\"black\" d=\"M117.09,-372.74C113.19,-337.93 109.83,-266.16 139,-216 171.84,-159.54 241.55,-124.18 287.57,-106.03\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"288.49,-109.04 296.59,-102.2 285.99,-102.5 288.49,-109.04\"/>\n</g>\n<!-- Valid\\nDataset&#45;&gt;Trainer -->\n<g id=\"edge13\" class=\"edge\">\n<title>Valid\\nDataset-&gt;Trainer</title>\n<path fill=\"none\" stroke=\"black\" d=\"M55.77,-294.03C56.56,-258.08 63.61,-184.14 106,-144 131.7,-119.66 228.56,-103.77 286.81,-96.19\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287.08,-99.56 296.56,-94.83 286.2,-92.61 287.08,-99.56\"/>\n</g>\n<!-- LR\\nScheduler -->\n<g id=\"node8\" class=\"node\">\n<title>LR\\nScheduler</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"342,-258.5 254,-258.5 254,-216 342,-216 342,-258.5\"/>\n<text text-anchor=\"middle\" x=\"298\" y=\"-241.2\" font-family=\"Times,serif\" font-size=\"14.00\">LR</text>\n<text text-anchor=\"middle\" x=\"298\" y=\"-223.95\" font-family=\"Times,serif\" font-size=\"14.00\">Scheduler</text>\n</g>\n<!-- Train\\nDataloader&#45;&gt;LR\\nScheduler -->\n<g id=\"edge8\" class=\"edge\">\n<title>Train\\nDataloader-&gt;LR\\nScheduler</title>\n<path fill=\"none\" stroke=\"black\" d=\"M295.8,-294.25C296.1,-286.7 296.44,-277.97 296.77,-269.68\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"300.3,-269.94 297.19,-259.81 293.3,-269.66 300.3,-269.94\"/>\n</g>\n<!-- Optimizers -->\n<g id=\"node10\" class=\"node\">\n<title>Optimizers</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"342.38,-180 247.62,-180 247.62,-144 342.38,-144 342.38,-180\"/>\n<text text-anchor=\"middle\" x=\"295\" y=\"-157.32\" font-family=\"Times,serif\" font-size=\"14.00\">Optimizers</text>\n</g>\n<!-- LR\\nScheduler&#45;&gt;Optimizers -->\n<g id=\"edge9\" class=\"edge\">\n<title>LR\\nScheduler-&gt;Optimizers</title>\n<path fill=\"none\" stroke=\"black\" d=\"M297.16,-215.85C296.85,-208.25 296.49,-199.49 296.16,-191.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"299.62,-191.17 295.71,-181.32 292.62,-191.45 299.62,-191.17\"/>\n</g>\n<!-- Custom\\nOptimizer -->\n<g id=\"node9\" class=\"node\">\n<title>Custom\\nOptimizer</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"235.62,-258.5 148.38,-258.5 148.38,-216 235.62,-216 235.62,-258.5\"/>\n<text text-anchor=\"middle\" x=\"192\" y=\"-241.2\" font-family=\"Times,serif\" font-size=\"14.00\">Custom</text>\n<text text-anchor=\"middle\" x=\"192\" y=\"-223.95\" font-family=\"Times,serif\" font-size=\"14.00\">Optimizer</text>\n</g>\n<!-- Custom\\nOptimizer&#45;&gt;Optimizers -->\n<g id=\"edge10\" class=\"edge\">\n<title>Custom\\nOptimizer-&gt;Optimizers</title>\n<path fill=\"none\" stroke=\"black\" d=\"M220.97,-215.65C233.63,-206.64 248.56,-196.02 261.69,-186.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"263.31,-189.12 269.43,-180.47 259.26,-183.41 263.31,-189.12\"/>\n</g>\n<!-- Optimizers&#45;&gt;Trainer -->\n<g id=\"edge11\" class=\"edge\">\n<title>Optimizers-&gt;Trainer</title>\n<path fill=\"none\" stroke=\"black\" d=\"M304.15,-143.7C308.27,-135.9 313.23,-126.51 317.82,-117.83\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"321.38,-119.58 322.96,-109.1 315.19,-116.31 321.38,-119.58\"/>\n</g>\n<!-- train -->\n<g id=\"node17\" class=\"node\">\n<title>train</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"359,-36 305,-36 305,0 359,0 359,-36\"/>\n<text text-anchor=\"middle\" x=\"332\" y=\"-13.32\" font-family=\"Times,serif\" font-size=\"14.00\">train</text>\n</g>\n<!-- Trainer&#45;&gt;train -->\n<g id=\"edge21\" class=\"edge\">\n<title>Trainer-&gt;train</title>\n<path fill=\"none\" stroke=\"black\" d=\"M332,-71.7C332,-64.24 332,-55.32 332,-46.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"335.5,-47.1 332,-37.1 328.5,-47.1 335.5,-47.1\"/>\n</g>\n<!-- Data\\nCollator -->\n<g id=\"node12\" class=\"node\">\n<title>Data\\nCollator</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"368.5,-415.5 295.5,-415.5 295.5,-373 368.5,-373 368.5,-415.5\"/>\n<text text-anchor=\"middle\" x=\"332\" y=\"-398.2\" font-family=\"Times,serif\" font-size=\"14.00\">Data</text>\n<text text-anchor=\"middle\" x=\"332\" y=\"-380.95\" font-family=\"Times,serif\" font-size=\"14.00\">Collator</text>\n</g>\n<!-- Data\\nCollator&#45;&gt;Train\\nDataloader -->\n<g id=\"edge14\" class=\"edge\">\n<title>Data\\nCollator-&gt;Train\\nDataloader</title>\n<path fill=\"none\" stroke=\"black\" d=\"M322.08,-372.75C318.27,-364.86 313.83,-355.68 309.66,-347.06\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"312.45,-345.79 304.94,-338.31 306.15,-348.83 312.45,-345.79\"/>\n</g>\n<!-- Data\\nCollator&#45;&gt;Trainer -->\n<g id=\"edge15\" class=\"edge\">\n<title>Data\\nCollator-&gt;Trainer</title>\n<path fill=\"none\" stroke=\"black\" d=\"M340.79,-372.56C344.77,-362.11 349.01,-349.1 351,-337 364.91,-252.36 365.69,-228.51 351,-144 349.51,-135.43 346.76,-126.41 343.81,-118.29\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"346.79,-117.3 339.89,-109.25 340.26,-119.84 346.79,-117.3\"/>\n</g>\n<!-- Tokenizer -->\n<g id=\"node13\" class=\"node\">\n<title>Tokenizer</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"483.5,-255.25 398.5,-255.25 398.5,-219.25 483.5,-219.25 483.5,-255.25\"/>\n<text text-anchor=\"middle\" x=\"441\" y=\"-232.57\" font-family=\"Times,serif\" font-size=\"14.00\">Tokenizer</text>\n</g>\n<!-- Tokenizer&#45;&gt;Trainer -->\n<g id=\"edge16\" class=\"edge\">\n<title>Tokenizer-&gt;Trainer</title>\n<path fill=\"none\" stroke=\"black\" d=\"M431.78,-218.84C421.3,-199.71 403.16,-168.45 384,-144 376.33,-134.21 367.06,-124.27 358.48,-115.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.45,-113.73 351.86,-109.23 356.56,-118.73 361.45,-113.73\"/>\n</g>\n<!-- Model -->\n<g id=\"node14\" class=\"node\">\n<title>Model</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"539.12,-180 478.88,-180 478.88,-144 539.12,-144 539.12,-180\"/>\n<text text-anchor=\"middle\" x=\"509\" y=\"-157.32\" font-family=\"Times,serif\" font-size=\"14.00\">Model</text>\n</g>\n<!-- Tokenizer&#45;&gt;Model -->\n<g id=\"edge17\" class=\"edge\">\n<title>Tokenizer-&gt;Model</title>\n<path fill=\"none\" stroke=\"black\" d=\"M457.12,-218.89C465.61,-209.74 476.18,-198.36 485.56,-188.25\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"487.53,-191.2 491.76,-181.49 482.4,-186.44 487.53,-191.2\"/>\n</g>\n<!-- Model&#45;&gt;Trainer -->\n<g id=\"edge18\" class=\"edge\">\n<title>Model-&gt;Trainer</title>\n<path fill=\"none\" stroke=\"black\" d=\"M478.57,-148.97C450.48,-137.86 408.4,-121.22 376.53,-108.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"378.16,-105.1 367.57,-104.67 375.58,-111.6 378.16,-105.1\"/>\n</g>\n<!-- LORA\\nSetting -->\n<g id=\"node15\" class=\"node\">\n<title>LORA\\nSetting</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"570.25,-258.5 501.75,-258.5 501.75,-216 570.25,-216 570.25,-258.5\"/>\n<text text-anchor=\"middle\" x=\"536\" y=\"-241.2\" font-family=\"Times,serif\" font-size=\"14.00\">LORA</text>\n<text text-anchor=\"middle\" x=\"536\" y=\"-223.95\" font-family=\"Times,serif\" font-size=\"14.00\">Setting</text>\n</g>\n<!-- LORA\\nSetting&#45;&gt;Model -->\n<g id=\"edge19\" class=\"edge\">\n<title>LORA\\nSetting-&gt;Model</title>\n<path fill=\"none\" stroke=\"black\" d=\"M528.48,-215.85C525.58,-207.99 522.23,-198.88 519.12,-190.46\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"522.13,-189.49 515.38,-181.32 515.56,-191.91 522.13,-189.49\"/>\n</g>\n<!-- BNB\\nSetting -->\n<g id=\"node16\" class=\"node\">\n<title>BNB\\nSetting</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"657.25,-258.5 588.75,-258.5 588.75,-216 657.25,-216 657.25,-258.5\"/>\n<text text-anchor=\"middle\" x=\"623\" y=\"-241.2\" font-family=\"Times,serif\" font-size=\"14.00\">BNB</text>\n<text text-anchor=\"middle\" x=\"623\" y=\"-223.95\" font-family=\"Times,serif\" font-size=\"14.00\">Setting</text>\n</g>\n<!-- BNB\\nSetting&#45;&gt;Model -->\n<g id=\"edge20\" class=\"edge\">\n<title>BNB\\nSetting-&gt;Model</title>\n<path fill=\"none\" stroke=\"black\" d=\"M590.94,-215.65C576.65,-206.47 559.76,-195.61 545.03,-186.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"547.49,-182.93 537.19,-180.47 543.71,-188.82 547.49,-182.93\"/>\n</g>\n</g>\n</svg>"},"metadata":{}}]},{"cell_type":"markdown","source":"Did you grasp a single thing?\n\nTrust me, I was just as puzzled initially. Perhaps, I still don't comprehend everything in-depth as of now. However, I'll make an attempt to shed light on it based on my current understanding.\n\nIf you think I am saying incorrect somewhere, please let me know in the comments.","metadata":{}},{"cell_type":"markdown","source":"## Installing required LLM Fine-tuning Libraries for Transformers and Quantization","metadata":{}},{"cell_type":"code","source":"!pip install trl==0.5.0\n!pip install peft==0.4.0\n!pip install torch==2.0.0\n!pip install wandb==0.15.8\n!pip install einops==0.6.1\n!pip install pandas==1.5.3\n!pip install datasets==2.1.0\n!pip install accelerate==0.20.3\n!pip install bitsandbytes==0.41.1\n!pip install nvidia-ml-py3==7.352.0\n!pip install huggingface_hub==0.16.4\n\n!pip install transformers==4.30.2\n!pip install transformers[torch]\n!pip install transformers[sentencepiece]\n\nfrom IPython.display import clear_output\nclear_output()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-28T18:24:02.471920Z","iopub.execute_input":"2023-10-28T18:24:02.472337Z","iopub.status.idle":"2023-10-28T18:26:54.510633Z","shell.execute_reply.started":"2023-10-28T18:24:02.472296Z","shell.execute_reply":"2023-10-28T18:26:54.509436Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Here's a breakdown of why each library is crucial for fine-tuning a large language model (LLM):\n\n**Core Libraries**:\n\n- trl: Transformer Reinforcement Learning (trl) provides essential training and evaluation tools for LLMs with Reinforcement Learning.\n\n- peft: Implements Parameter Efficient Fine-Tuning (PEFT), a technique that improves efficiency and accuracy by fine-tuning a small number of model parameters while freezing most parameters of the pretrained LLM, thereby greatly decreasing the computational and storage costs.\n\n- transformers: The core library for working with transformer models, including BERT, GPT-3, and many others, providing a wide range of pre-trained models and model architectures.\n\n- torch: The core machine learning library, providing tensor computations, automatic differentiation, and deep learning building blocks.\n\n- transformers[torch]: Ensures compatibility between the Transformers library and PyTorch for seamless integration.\n\n- transformers[sentencepiece]: Installs SentencePiece, a powerful text tokenizer often used with transformer models for efficient text segmentation and tokenization.\n\n\n**Optimization and Efficiency**:\n\n- wandb: Enables logging, visualization, and tracking of experiments for better understanding and optimization of model training.\n\n- einops: Simplifies tensor manipulations and rearrangements, often used in LLM architectures for efficient computations.\n\n- pandas: Facilitates data analysis and manipulation for preparing and preprocessing text datasets effectively.\n\n- datasets: Offers a flexible framework for loading, managing, and transforming different dataset formats, ensuring compatibility with various data sources.\n\n- accelerate: Optimizes training for large models and datasets, leveraging distributed training and hardware acceleration for faster processing.\n\n- bitsandbytes: Enables model quantization, reducing model size and memory footprint for more efficient deployment and inference.\n\n**Hardware Acceleration and Model Deployment**:\n\n- nvidia-ml-py3: Provides tools and libraries for working with NVIDIA GPUs, optimizing performance for LLM training and inference.\n\n- huggingface_hub: Connects to the Hugging Face Hub for accessing pre-trained models, sharing fine-tuned models, and collaborating with the LLM community.","metadata":{}},{"cell_type":"markdown","source":"## Importing Required Libraries and Modules","metadata":{}},{"cell_type":"code","source":"import gc\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', 500)\n\nfrom peft import PeftModel\nfrom peft import LoraConfig\nfrom peft import get_peft_model\nfrom peft import AutoPeftModelForCausalLM\nfrom peft import prepare_model_for_kbit_training\n\nimport transformers\nfrom transformers import Trainer\nfrom transformers import AutoTokenizer\nfrom transformers import TrainingArguments\nfrom transformers import BitsAndBytesConfig\nfrom transformers import AutoModelForCausalLM\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers.trainer_pt_utils import get_parameter_names\n\nimport trl\nfrom trl import SFTTrainer\nfrom trl import DataCollatorForCompletionOnlyLM\n\nimport huggingface_hub\nimport bitsandbytes as bnb\nfrom accelerate import Accelerator\nfrom accelerate import DistributedType\nfrom accelerate import notebook_launcher\nfrom accelerate.utils import set_seed as asd\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data.dataloader import DataLoader\n\nimport wandb\nfrom pynvml import *\nfrom tqdm import tqdm\nfrom typing import List\nfrom datasets import Dataset\nfrom datasets import load_dataset\nfrom dataclasses import dataclass\nfrom IPython.display import display\nfrom IPython.display import FileLink\nfrom kaggle_secrets import UserSecretsClient\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:26:54.516249Z","iopub.execute_input":"2023-10-28T18:26:54.516521Z","iopub.status.idle":"2023-10-28T18:27:01.579097Z","shell.execute_reply.started":"2023-10-28T18:26:54.516492Z","shell.execute_reply":"2023-10-28T18:27:01.578096Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Using Secret Ingredients 🤫","metadata":{}},{"cell_type":"code","source":"# Fetch Secrets\nuser_secrets=UserSecretsClient()\n\n# Fetch secret keys\nmy_secret_hf_api_key_read=user_secrets.get_secret(\"hf_login\")\nmy_secret_hf_api_key_write=user_secrets.get_secret(\"hf_model_write\")\nmy_secret_wandb_api_key=user_secrets.get_secret(\"wandb_api_key\")\n\n# Use the secret keys to login\nwandb.login(key=my_secret_wandb_api_key)\nhuggingface_hub.login(token=my_secret_hf_api_key_read, add_to_git_credential=False)\nhuggingface_hub.login(token=my_secret_hf_api_key_write, add_to_git_credential=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:01.580221Z","iopub.execute_input":"2023-10-28T18:27:01.580844Z","iopub.status.idle":"2023-10-28T18:27:04.960947Z","shell.execute_reply.started":"2023-10-28T18:27:01.580815Z","shell.execute_reply":"2023-10-28T18:27:04.959913Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityadawn98\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"markdown","source":"### GPU Gauntlet: Unveiling the Power of Your Graphics Colossus","metadata":{}},{"cell_type":"markdown","source":"Imagine whipping up a culinary masterpiece in a bustling kitchen. We've gathered the finest ingredients—text and code—and set the recipe—the language model—to simmer. But just like a chef keeps an eye on the oven's temperature, we need tools to monitor our model's training progress and ensure it's cooking up brilliance efficiently.\n\nEnter these vigilant kitchen assistants, crafted in code:\n\nThe Attentive Sous-Chef: ```print_gpu_utilization()```\n\n- \"How's our workspace looking, chef?\"\n- This function peeks into the GPU pantry, checking how much memory is being used.\n- It's like checking your oven space—ensuring there's room for your creation to rise and avoiding any spills or burnt edges.\n\nThe Diligent Head Chef: ```print_summary()```\n\n- \"How's the dish progressing? Give me the details!\"\n- This function delivers a concise report on the training process, like a chef tasting and evaluating a dish.\n- It reveals the time spent training, the speed of learning (samples per second), and, of course, the current GPU memory usage.\n- It ensures the model is learning effectively and efficiently, preventing any undercooked or overcooked results.\n\nTogether, these functions ensure that our language model training is a smooth, well-orchestrated symphony in the kitchen of artificial intelligence. They help us create the most flavorful and intelligent language models possible, ready to serve up delicious insights and conversations. Bon appétit!","metadata":{}},{"cell_type":"code","source":"try:\n    def print_gpu_utilization():\n        \"\"\"\n        Prints the current GPU memory usage in MB.\n\n        Uses the NVIDIA Management Library (NVML) to retrieve GPU information.\n        \"\"\"\n        nvmlInit()\n        handle = nvmlDeviceGetHandleByIndex(0)\n        info = nvmlDeviceGetMemoryInfo(handle)\n        print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n    def print_summary(result):\n        \"\"\"\n       Prints a summary of training results, including time, samples per second, and GPU memory usage.\n\n       Args:\n           result: A dictionary containing training metrics, including 'train_runtime' and 'train_samples_per_second'.\n\n       Raises:\n           RuntimeError: If a GPU is not found.\n       \"\"\"\n        print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n        print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n        print_gpu_utilization()\nexcept:\n    print('GPU not found!')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:04.984989Z","iopub.execute_input":"2023-10-28T18:27:04.985608Z","iopub.status.idle":"2023-10-28T18:27:04.997091Z","shell.execute_reply.started":"2023-10-28T18:27:04.985576Z","shell.execute_reply":"2023-10-28T18:27:04.996297Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Seeding the Future: Planting the Right Randomness for Reproducing Results ","metadata":{}},{"cell_type":"markdown","source":"And now, meet the meticulous recipe curator: ```fix_all_seeds()```\n\nImagine this as the chef who insists on precise measurements and timing for every ingredient.\n\n- \"A pinch of randomness here, a dash of data there—but always with precision!\"\n- This function ensures that every experiment in the kitchen of AI follows a consistent recipe, guaranteeing reproducible results.\n- It's like using the same measuring cups and spoons every time you bake a cake—no matter who's in the kitchen, the cake will always turn out delicious.\n\nHow does it work?\n\n- Aligning the Kitchen Clocks: It sets the same random seed across multiple libraries (accelerate, Python's random, TRL, NumPy, PyTorch) and environments (CPU and CUDA), ensuring all ingredients blend in perfect harmony.\n- Organizing the Pantry: It even controls the order in which ingredients are picked (the Python hash seed), preventing any unexpected surprises in the final dish.\n\nThe result?\n\n- Perfectly Replicable Recipes: With fix_all_seeds(), we can confidently share and replicate our LLM experiments, knowing that others can recreate the same delicious results in their own AI kitchens.\n- No More \"Oops, I Forgot to Write Down That Step\" Moments: It eliminates the frustration of unexpected variations due to random chance, ensuring that our language models are always cooking up brilliance, consistently and predictably.","metadata":{}},{"cell_type":"code","source":"def fix_all_seeds(seed):\n    \"\"\"\n    Sets the random seeds for various libraries and environments to ensure reproducibility.\n\n    Args:\n    - seed (int): The desired seed value to use for all random number generators.\n\n    Notes:\n    - Sets seeds for:\n        - accelerate library\n        - Python's `random` module\n        - The `trl` library\n        - NumPy's random number generator\n        - PyTorch's CPU and CUDA random number generators\n        - The Python hash seed for dictionary ordering\n    \"\"\"\n    asd(seed)\n    random.seed(seed)\n    trl.set_seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:04.998093Z","iopub.execute_input":"2023-10-28T18:27:04.998363Z","iopub.status.idle":"2023-10-28T18:27:05.008155Z","shell.execute_reply.started":"2023-10-28T18:27:04.998340Z","shell.execute_reply":"2023-10-28T18:27:05.007293Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Data Deluge: Gathering the Fuel for Your Language Learning Machine","metadata":{}},{"cell_type":"markdown","source":"Now, let's welcome the skilled forager and pantry organizer: ```fetch_datasets()```\n\nImagine this as the dedicated kitchen assistant who gathers the freshest ingredients from the market and arranges them neatly in the pantry, ready for the chef's magic touch.\n\n- \"What's on the menu today, chef? I'll fetch the finest ingredients!\"\n- This function gracefully retrieves the training and validation datasets (the flavorful text data that nourishes our language model), ensuring they're accessible and ready to use.\n\nHere's how it orchestrates the pantry:\n\n- Gathering the Goods: It retrieves the paths to the training and validation CSV files, where our text ingredients are stored.\n- Preparing for Efficiency: It checks if we need to create iterable datasets, which is like arranging ingredients in easy-to-grab containers to save space and time during cooking.\n- Unpacking the Deliveries: It loads the datasets, either as iterable or standard, depending on our efficiency needs.\n\nThe result?\n\n- A Well-Stocked Pantry: With ```fetch_datasets()```, the language model kitchen has all the essential ingredients within arm's reach, ready to be transformed into delicious language creations.\n- Efficient Workflow: By optimizing dataset handling, it prevents any unnecessary delays or spills, ensuring a smooth and productive cooking experience.","metadata":{}},{"cell_type":"code","source":"def fetch_datasets(Config_Dataset):\n    \"\"\"\n    Loads and returns training and validation datasets from CSV files based on configuration settings.\n\n    Args:\n    - Config_Dataset: A configuration object containing the following attributes:\n        - TRAIN_CSV_PATH (str): Path to the training CSV file.\n        - VALID_CSV_PATH (str): Path to the validation CSV file.\n        - CREATE_ITERABLE (bool): Whether to create iterable datasets for memory efficiency.\n\n    Returns:\n    - tuple: A tuple containing the loaded training and validation datasets.\n\n    Raises:\n    - Exception: Raises any exceptions that occur during dataset loading.\n    \"\"\"\n    \n    try:\n        data_files = {\n                        \"train\": [Config_Dataset.TRAIN_CSV_PATH],\n                        \"valid\": [Config_Dataset.VALID_CSV_PATH]\n                    }\n\n        if Config_Dataset.CREATE_ITERABLE:\n            dataset = load_dataset('csv', data_files=data_files, streaming=True)\n            train_dataset = dataset['train']\n            valid_dataset = dataset['valid']\n\n        else:\n            dataset = load_dataset('csv', data_files=data_files)\n            train_dataset = dataset['train']\n            valid_dataset = dataset['valid']\n\n        del Config_Dataset\n        gc.collect()\n\n        return train_dataset, valid_dataset\n\n    except Exception as e:\n        raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.009299Z","iopub.execute_input":"2023-10-28T18:27:05.009836Z","iopub.status.idle":"2023-10-28T18:27:05.020546Z","shell.execute_reply.started":"2023-10-28T18:27:05.009803Z","shell.execute_reply":"2023-10-28T18:27:05.019730Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Batch Bonanza: Crafting the Ideal Data Packages ","metadata":{}},{"cell_type":"markdown","source":"Meet the masterful plate assembler: ```data_loader()```\n\nImagine this as the skilled maître d' who artfully arranges each dish, ensuring the perfect portion sizes and presentation for every course of the meal.\n\n- \"Dinner is served! Each course, meticulously prepared and ready to delight the mind.\"\n- This function takes the carefully gathered ingredients (our text data) and creates a delightful procession of bite-sized samples, perfectly portioned for our language model to savor and learn from.\n\nHere's how it orchestrates the feast:\n\n- Setting the Table: It takes the dataset—our pantry of text ingredients—and prepares to serve it up in an organized manner.\n- Crafting the Perfect Bite: It determines the ideal batch size, which is like deciding how many delectable morsels to place on each plate. Too little, and the meal feels incomplete; too much, and it overwhelms the palate.\n- Calling for Staff: It enlists the help of multiple worker processes to efficiently fetch and prepare the data, like a team of waiters working in harmony to ensure a seamless dining experience.\n- Pinning for Speed: It optimizes data transfer to the GPU, like a maître d' clearing a direct path from the kitchen to the table for swift and smooth delivery.\n- Customizing the Presentation: If needed, it allows for a custom collation function, like a chef offering tailored plating options to suit different tastes and preferences.\n\nThe result?\n\n- A Symphony of Flavors: With ```data_loader()```, our language model savors each course of text data in well-balanced portions, ensuring optimal learning and digestion.\n- Efficient Service, Elevated Experience: By optimizing data preparation and delivery, it prevents any delays or hiccups, creating a seamless and enjoyable learning experience for our model.","metadata":{}},{"cell_type":"code","source":"def data_loader(\n        dataset,\n        batch_size,\n        num_workers,\n        pin_memory,\n        collate_fn=None\n    ):\n    \"\"\"\n    Creates a PyTorch DataLoader for a given dataset with specified parameters.\n\n    Args:\n    - dataset (torch.utils.data.Dataset): The dataset to load.\n    - batch_size (int): The number of samples to include in each batch.\n    - num_workers (int): The number of worker processes to use for loading data.\n    - pin_memory (bool): Whether to pin memory for faster data transfer to GPU.\n    - collate_fn (callable, optional): A function to customize the collation of samples into batches. Defaults to None.\n\n    Returns:\n    - torch.utils.data.DataLoader: The created DataLoader.\n    \"\"\"\n    return DataLoader(\n                dataset=dataset,\n                batch_size=batch_size,\n                num_workers=num_workers,\n                pin_memory=pin_memory,\n                collate_fn=collate_fn\n            )","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.021733Z","iopub.execute_input":"2023-10-28T18:27:05.022046Z","iopub.status.idle":"2023-10-28T18:27:05.035668Z","shell.execute_reply.started":"2023-10-28T18:27:05.022022Z","shell.execute_reply":"2023-10-28T18:27:05.034954Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_model_config(peft_model_id: str):\n    return LoraConfig.from_pretrained(peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.036707Z","iopub.execute_input":"2023-10-28T18:27:05.036992Z","iopub.status.idle":"2023-10-28T18:27:05.047269Z","shell.execute_reply.started":"2023-10-28T18:27:05.036968Z","shell.execute_reply":"2023-10-28T18:27:05.046470Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Breaking Down the Language Barrier: Introducing the Word Maestro","metadata":{}},{"cell_type":"markdown","source":"Now, let's welcome the master knifesmith and flavor architect: ```get_tokenizer()```\n\nImagine this as the skilled artisan who crafts the perfect cutlery for our language model, ensuring each ingredient's distinct flavors are precisely cut, seasoned, and ready to blend harmoniously.\n\n- \"Every ingredient deserves a perfect cut, every nuance a sharpened edge.\"\n- This function retrieves the essential tokenizer—the tool that dissects language into its fundamental building blocks, much like a chef's knife slicing and dicing vegetables for a balanced dish.\n\nHere's how it shapes the language experience:\n\n- Visiting the Hugging Face Pantry: It journeys to the Hugging Face Hub, a vast repository of pre-trained models and tokenizers, like a chef exploring a bustling market for the finest tools of the trade.\n- Selecting the Right Knife: It chooses the tokenizer that matches our chosen pre-trained model, understanding that different cuisines (models) require different precision instruments.\n- Customizing the Cut: It empowers us to configure the tokenizer's behavior, like adjusting the blade's sharpness or angle for specific ingredients. We can choose whether to apply padding or truncation.\n- Handling Special Ingredients: It ensures that any special tokens, like those marking the end of sentences or filling extra space, are appropriately handled, like a chef carefully adjusting cooking times for delicate herbs or spices.\n\nThe result?\n\n- Precision and Clarity: With ```get_tokenizer()```, our language model savors each ingredient (word or phrase) in its most refined form, understanding their distinct flavors and relationships, leading to richer and more nuanced language creations.\n- Flexibility and Control: By allowing us to customize the tokenizer's behavior, we can tailor the learning process to suit different language styles and preferences, much like a chef adapting a recipe to accommodate different tastes or dietary needs.","metadata":{}},{"cell_type":"code","source":"def get_tokenizer(MODEL_ID,Config_Tokenizer):\n    \"\"\"\n    Loads a tokenizer for a pre-trained model from the Hugging Face Hub, applying specified configurations.\n\n    Args:\n    - MODEL_ID (str): The identifier of the pre-trained model to load the tokenizer for.\n    - Config_Tokenizer: A configuration object containing settings for loading the tokenizer, with attributes:\n        - PADDING (bool): Whether to apply padding to inputs.\n        - TRUNCATION (bool): Whether to apply truncation to inputs.\n        - USE_AUTH_TOKEN (bool): Whether to use an authentication token for model loading.\n        - FORCE_DOWNLOAD (bool): Whether to force a fresh download of the tokenizer.\n        - RESUME_DOWNLOAD (bool): Whether to resume a previously interrupted download.\n        - TRUST_REMOTE_CODE (bool): Whether to trust remote code when loading the tokenizer.\n        - ADD_SPECIAL_TOKENS (bool): Whether to add special tokens to the vocabulary.\n\n    Returns:\n    - transformers.AutoTokenizer: The loaded and configured tokenizer.\n\n    Raises:\n    - Exception: Raises any exceptions that occur during tokenizer loading.\n    \"\"\"\n    try:\n        # Fetch Tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n                        MODEL_ID,\n                        padding=Config_Tokenizer.PADDING,\n                        truncation=Config_Tokenizer.TRUNCATION,\n                        use_auth_token=Config_Tokenizer.USE_AUTH_TOKEN,\n                        force_download=Config_Tokenizer.FORCE_DOWNLOAD,\n                        resume_download=Config_Tokenizer.RESUME_DOWNLOAD,\n                        trust_remote_code=Config_Tokenizer.TRUST_REMOTE_CODE,\n                        add_special_tokens=Config_Tokenizer.ADD_SPECIAL_TOKENS\n                    )\n\n        # Configure EOS-Tokens\n        if tokenizer.eos_token is None:\n            tokenizer.eos_token = '[EOS]'\n\n        # Configure PAD-Tokens\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n\n        return tokenizer\n\n    except Exception as e:\n        raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.048256Z","iopub.execute_input":"2023-10-28T18:27:05.048524Z","iopub.status.idle":"2023-10-28T18:27:05.060079Z","shell.execute_reply.started":"2023-10-28T18:27:05.048501Z","shell.execute_reply":"2023-10-28T18:27:05.059231Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Painting with Bits: Mastering Quantization for Efficient Computation","metadata":{}},{"cell_type":"markdown","source":"Now, let's meet the masterful alchemist and flavor preserver: ```config_bnb()```\n\nImagine this as the chef who carefully crafts delicate reductions and essences, capturing the vibrant flavors of our ingredients while preserving them in a more concentrated form.\n\n- \"Concentrating the essence, preserving the flavor, ensuring every nuance endures.\"\n- This function configures quantization, a technique that distills our model's knowledge into a smaller, more efficient format, much like a chef reducing a sauce to intensify its flavors while using fewer resources.\n\nHere's how it brews the magic:\n\n- Setting the Precision: It determines the level of quantization, like adjusting the intensity of a reduction. We can choose to load the model in 4-bit precision, a smaller size that often retains much of the flavor while using less storage and energy.\n- Selecting the Technique: It chooses the specific quantization method, like selecting the perfect cooking vessel for a reduction. Different techniques, such as \"nf4\" or others, can yield varying results, depending on the model and desired outcomes.\n- Choosing the Compute Type: It determines the underlying data type used during computations, like adjusting the heat level for a reduction. Different compute dtypes, like \"torch.bfloat16\", can balance precision and efficiency, ensuring both accuracy and resource conservation.\n- Double Quantization: It offers the option of double quantization, like a chef performing a multi-step reduction for the most concentrated essence. This technique can further reduce model size and accelerate performance, but it requires careful attention to detail.\n\nThe result?\n\n- Efficiency and Preservation: With ```config_bnb()```, our language model's knowledge is distilled into a more compact and efficient form, allowing it to run faster and consume less memory, like a reduced sauce that packs a flavorful punch without taking up too much space on the plate.\n- Flavor Control: By adjusting quantization settings, we can fine-tune the balance between model size, speed, and accuracy, much like a chef carefully adjusting the intensity of a reduction to achieve the perfect balance of flavors.","metadata":{}},{"cell_type":"code","source":"def config_bnb(Config_BNB):\n    \"\"\"\n    Creates a BitsAndBytesConfig object for quantization settings, based on provided configuration values.\n\n    Args:\n    - Config_BNB: A configuration object containing quantization settings, with attributes:\n        - LOAD_IN_4BIT (bool): Whether to load the model in 4-bit precision.\n        - BNB_4BIT_QUANT_TYPE (str): The type of 4-bit quantization to use (e.g., \"nf4\").\n        - BNB_4BIT_COMPUTE_DTYPE (str): The compute dtype to use for 4-bit quantization (e.g., \"torch.bfloat16\").\n        - BNB_4BIT_USE_DOUBLE_QUANT (bool): Whether to use double quantization for 4-bit precision.\n\n    Returns:\n    - The created BitsAndBytesConfig object.\n\n    Raises:\n    - Exception: Raises any exceptions that occur during configuration creation.\n    \"\"\"\n    \n    # Create BitsAndBytes Configurations\n    try:\n        return BitsAndBytesConfig(\n                    load_in_4bit=Config_BNB.LOAD_IN_4BIT,\n                    bnb_4bit_quant_type=Config_BNB.BNB_4BIT_QUANT_TYPE,\n                    bnb_4bit_compute_dtype=Config_BNB.BNB_4BIT_COMPUTE_DTYPE,\n                    bnb_4bit_use_double_quant=Config_BNB.BNB_4BIT_USE_DOUBLE_QUANT\n                )\n    except Exception as e:\n        raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.061092Z","iopub.execute_input":"2023-10-28T18:27:05.061381Z","iopub.status.idle":"2023-10-28T18:27:05.075353Z","shell.execute_reply.started":"2023-10-28T18:27:05.061330Z","shell.execute_reply":"2023-10-28T18:27:05.074654Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Lora's Low-Rank Legacy: Building a Leaner, Meaner Language Model","metadata":{}},{"cell_type":"markdown","source":"Now, let's introduce the master architect and flavor sculptor: ```config_lora()```\n\nImagine this as the chef who masterfully designs intricate structures and textures within a dish, ensuring that each flavor is not only present but also arranged in a way that enhances its impact and creates a harmonious symphony on the palate.\n\n- \"Every ingredient has a role to play, a note to contribute. Let's orchestrate a masterpiece of flavor and efficiency.\"\n- This function configures LORA, a technique that optimizes our model's structure for better performance and efficiency, much like a chef carefully layering flavors and textures to create a dish that is both delicious and satisfying without being overwhelming.\n\nHere's how it shapes the culinary experience:\n\n- Setting the Rank: It determines the level of compression applied to the model's weight matrices, like adjusting the density of a cake batter. A lower rank often leads to a more compact and efficient model, while a higher rank can maintain more detail and accuracy, depending on the recipe's needs.\n- Adding Bias: It decides whether to incorporate bias terms into the low-rank approximation, like a chef deciding whether to add a pinch of salt to enhance the flavors of a dish. Bias can improve model performance in certain cases, but it needs to be balanced carefully.\n- Specifying the Task: It aligns model configuration with the specific culinary task at hand, like a chef choosing the right tools and techniques for baking versus sautéing. Different tasks, such as \"CAUSAL_LM\" or others, may require different LORA settings for optimal results.\n- Regularizing with Alpha: It introduces a regularization parameter to prevent overfitting, much like a chef adding a touch of lemon juice to prevent a cream sauce from becoming too heavy. This helps the model generalize better to new data, ensuring it doesn't become too reliant on the training examples.\n- Applying Dropout: It removes connections between neurons during training, like a chef experimenting with different combinations of ingredients to discover surprising flavor pairings. This technique can help prevent overfitting and encourage the model to learn more robust representations of language.\n\nThe result?\n\n- Optimized Structure, Enhanced Flavor: With ```config_lora()```, our language model becomes more efficient and capable of handling complex tasks without sacrificing the richness of language understanding. It's like creating a dish that is both light and flavorful, satisfying without being overly indulgent.\n- Fine-Tuning for Balance: By adjusting LORA configuration, we can control the trade-offs between accuracy, model size, and computational cost, finding the perfect balance that suits our specific needs and resources. It's like a chef adjusting the heat, the timing, and the proportions of ingredients to create a dish that is perfectly cooked and balanced in flavor","metadata":{}},{"cell_type":"code","source":"def config_lora(Config_LORA):\n    \"\"\"\n    Creates a LoraConfig object for LORA-specific model configuration, based on provided values.\n\n    Args:\n    - Config_LORA: A configuration object containing LORA settings, with attributes:\n        - LORA_R (int): The rank of the low-rank approximation.\n        - LORA_BIAS (str): Bias to include in the low-rank approximation.\n        - TASK_TYPE (str): The type of task the model is being used for (e.g., \"CAUSAL_LM\").\n        - LORA_ALPHA (float): The LORA alpha parameter for regularization.\n        - LORA_DROPOUT (float): The dropout probability to apply in LORA layers.\n\n    Returns:\n    - The created LoraConfig object.\n\n    Raises:\n    - Exception: Raises any exceptions that occur during configuration creation.\n    \"\"\"\n    try:\n        return LoraConfig(\n                    r=Config_LORA.LORA_R,\n                    bias=Config_LORA.LORA_BIAS,\n                    task_type=Config_LORA.TASK_TYPE,\n                    lora_alpha=Config_LORA.LORA_ALPHA,\n                    lora_dropout=Config_LORA.LORA_DROPOUT,\n                )\n    except Exception as e:\n        raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.076385Z","iopub.execute_input":"2023-10-28T18:27:05.076644Z","iopub.status.idle":"2023-10-28T18:27:05.086283Z","shell.execute_reply.started":"2023-10-28T18:27:05.076622Z","shell.execute_reply":"2023-10-28T18:27:05.085352Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Make the Model Monster: Crafting a Quantized Language Leviathan","metadata":{}},{"cell_type":"markdown","source":"Now, let's welcome the grand maître d' and master weaver: ```make_model()```\n\nImagine this as the head chef who artfully assembles and aligns every element of the kitchen, ensuring that the finest ingredients, specialized techniques, and precision tools work in perfect harmony to create a symphony of flavors and efficiency.\n\n- \"Every detail matters, every ingredient has its place. Let's orchestrate a culinary masterpiece that delights the mind and respects our resources.\"\n- This function constructs the heart of our language model, carefully selecting, configuring, and optimizing its components for peak performance and resource consciousness, much like a chef designing a menu that balances bold flavors with sustainable practices.\n\nHere's how it conducts the culinary ballet:\n\n- Gathering the Essential Ingredients: It fetches a PEFT-enabled model from the Hugging Face Hub, a treasure trove of pre-trained culinary knowledge, like a chef sourcing exquisite ingredients from trusted vendors.\n- Quantizing for Flavor Concentration: It applies quantization techniques, carefully reducing the model's size and resource requirements without sacrificing too much of its expressiveness, much like a chef intensifying flavors through reductions and essences.\n- Managing the Kitchen Space: It optimizes memory usage across multiple GPUs, ensuring efficient use of resources, like a chef streamlining workflows and optimizing storage to accommodate elaborate dishes within a bustling kitchen.\n- Preparing for k-bit Training: It further refines the model for k-bit training, a specialized technique that unlocks even greater efficiency, like a chef experimenting with innovative cooking methods to achieve new levels of resourcefulness.\n- Infusing with PEFT Magic: It incorporates PEFT, a powerful approach that enhances model performance and adaptability, much like a chef adding secret spices and innovative techniques to create dishes that surprise and delight.\n- Assigning to the Right Oven: It places the meticulously crafted model onto the designated device (GPU or CPU), like a chef choosing the perfect oven to ensure optimal cooking conditions.\n\nThe result?\n\n- A Masterpiece of Intelligence and Efficiency: With ```make_model()```, we have a language model that is not only capable of generating delicious language but also able to do so while respecting our computational resources, like a chef crafting exquisite meals that both captivate the senses and embrace sustainability.\n- Balance and Harmony: This function exemplifies the artful balance between language understanding, model efficiency, and resource management, ensuring that our language model can continue to learn and evolve without overwhelming our kitchen's capacity.","metadata":{}},{"cell_type":"code","source":"def make_model(\n        N_GPUS,\n        DEVICE,\n        MODEL_ID,\n        tokenizer,\n        MAX_MEMORY,\n        BNB_Config,\n        PEFT_Config,\n        process_index,\n        sequence_max_length,\n        torch_dtype_from_bnb_config\n    ):\n    \"\"\"\n    Loads a PEFT-enabled model from the Hugging Face Hub, applies quantization and memory optimizations,and prepares it for k-bit training.\n\n    Args:\n    - N_GPUS (int): The number of available GPUs.\n    - DEVICE (str): The device to load the model onto (e.g., \"cuda\" or \"cpu\").\n    - MODEL_ID (str): The identifier of the pre-trained model to load.\n    - tokenizer: The tokenizer associated with the pre-trained model.\n    - MAX_MEMORY (int): The maximum memory per GPU to allocate for the model.\n    - BNB_Config: A BitsAndBytesConfig object for quantization settings.\n    - PEFT_Config: A configuration object for PEFT settings (details unspecified).\n    - process_index (int): The index of the current process in distributed training.\n    - sequence_max_length (int): The maximum sequence length to accommodate.\n    - torch_dtype_from_bnb_config (torch.dtype): The torch dtype to use based on quantization settings.\n\n    Returns:\n    - AutoPeftModelForCausalLM: The loaded, quantized, optimized, and PEFT-enabled model.\n\n    Raises:\n    - Exception: Raises any exceptions that occur during model loading or configuration.\n    \"\"\"\n    try:\n        model = AutoPeftModelForCausalLM.from_pretrained(\n                    MODEL_ID,\n                    trust_remote_code=True,\n                    device_map=\"auto\", # {\"\": process_index},\n                    quantization_config=BNB_Config,\n                    torch_dtype=torch_dtype_from_bnb_config,\n                    max_memory = {i: MAX_MEMORY for i in range(N_GPUS)}\n                )\n        model.config.use_cache = False\n        model.gradient_checkpointing_enable()\n        try:\n            model.resize_token_embeddings(len(tokenizer))\n        except:\n            model.resize_token_embeddings(sequence_max_length)\n        model = prepare_model_for_kbit_training(model)\n        model = get_peft_model(model, PEFT_Config)\n        model.to(DEVICE)\n        return model\n    except Exception as e:\n        raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.090601Z","iopub.execute_input":"2023-10-28T18:27:05.090988Z","iopub.status.idle":"2023-10-28T18:27:05.100293Z","shell.execute_reply.started":"2023-10-28T18:27:05.090965Z","shell.execute_reply":"2023-10-28T18:27:05.099292Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Fueling the Machine: Tailoring the Optimizer for Peak Performance","metadata":{}},{"cell_type":"markdown","source":"Now, let's welcome the seasoned sous chef and flavor guide: ```Create_Optimizer()```\n\nImagine this as the experienced sous chef who carefully crafts the perfect blend of seasonings and techniques to guide the learning process, ensuring that our language model evolves towards a harmonious balance of flavors, textures, and efficiency, much like a baker adjusting the leavening agents and baking time to create a cake that is both light and flavorful.\n\n- \"Every ingredient needs a guiding hand, every flavor a direction to blossom. Let's create the perfect conditions for growth and refinement.\"\n- This function tailors an optimizer, a tool that governs how our model learns from experience, adjusting its internal parameters to capture the nuances of language and achieve its full potential.\n\nHere's how it orchestrates the learning journey:\n\n- Understanding the Ingredients: It carefully examines the model's architecture, identifying the distinct components that require different levels of guidance, like a baker adjusting the amount of sugar and flour based on the type of cake being made.\n- Applying Weight Decay: It introduces a technique that prevents certain model components from becoming too dominant, ensuring a balanced flavor profile, like a chef using a light touch with potent spices to avoid overpowering a dish.\n- Selecting the AdamW Method: It chooses the AdamW optimizer, a proven approach for effectively guiding model learning, like a baker relying on a trusted recipe for a consistently delicious cake.\n- Setting the Learning Pace: It establishes the learning rate, controlling the speed at which the model adjusts its knowledge, much like a baker regulating the oven temperature to achieve a perfect bake.\n- Incorporating Quantization: It integrates quantization awareness, ensuring that the optimizer works harmoniously with the model's compressed format, like a baker adjusting the recipe for a smaller cake pan without sacrificing taste.\n- Enhancing Precision (if available): It offers the option to further refine quantization within the optimizer itself, potentially unlocking even greater efficiency, like a baker experimenting with different types of flour to achieve a finer crumb.\n\nThe result?\n\n- Steady Growth and Refinement: With ```Create_Optimizer()```, our language model embarks on a focused learning journey, guided by a meticulously crafted plan that balances flavor development with resource awareness, like a baker carefully nurturing a sourdough starter to achieve both complexity and efficiency.\n- Tailored Guidance for Each Ingredient: This function ensures that each component of our model receives the optimal level of attention, preventing any single flavor from overshadowing the others and fostering a symphony of language understanding that is both rich and refined.","metadata":{}},{"cell_type":"code","source":"def Create_Optimizer(\n        model,\n        optim_bits,\n        adam_beta1,\n        adam_beta2,\n        adam_epsilon,\n        weight_decay,\n        learning_rate\n    ):\n    \"\"\"\n    Creates an AdamW optimizer with BitsAndBytes quantization support, applying different weight decay settings.\n\n    Args:\n    - model: The model to create the optimizer for.\n    - optim_bits (int): The number of bits to use for quantization in the optimizer (if supported).\n    - adam_beta1 (float): The first beta parameter for AdamW.\n    - adam_beta2 (float): The second beta parameter for AdamW.\n    - adam_epsilon (float): The epsilon parameter for AdamW.\n    - weight_decay (float): The weight decay coefficient to apply.\n    - learning_rate (float): The learning rate for the optimizer.\n\n    Returns:\n    - bnb.optim.AdamW: The created optimizer.\n\n    Raises:\n    - Exception: Re-raises any exceptions that occur during optimizer creation.\n    \"\"\"\n    try:\n        decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n                \"weight_decay\": weight_decay,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n        optimizer_kwargs = {\n            \"betas\": (adam_beta1, adam_beta2),\n            \"eps\": adam_epsilon,\n        }\n\n        optimizer_kwargs[\"lr\"] = learning_rate\n        return bnb.optim.AdamW(\n                    optimizer_grouped_parameters,\n                    lr=learning_rate,\n                    eps=adam_epsilon,\n#                     optim_bits=optim_bits,\n                    betas=(adam_beta1, adam_beta2)\n                )\n    except Exception as e:\n        raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.101391Z","iopub.execute_input":"2023-10-28T18:27:05.101717Z","iopub.status.idle":"2023-10-28T18:27:05.116115Z","shell.execute_reply.started":"2023-10-28T18:27:05.101684Z","shell.execute_reply":"2023-10-28T18:27:05.115214Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### ⏱️ Mastering the Pace: Scheduling Success for AI's Learning Journey","metadata":{}},{"cell_type":"markdown","source":"Now, let's welcome the watchful pastry chef and timekeeper: ```Create_Scheduler()```\n\nImagine this as the meticulous pastry chef who carefully regulates the oven's temperature and bake time, ensuring that each stage of the learning process unfolds at the perfect pace, much like adjusting the oven settings to create a croissant that is both flaky and tender.\n\n- \"Every ingredient has its moment to shine, every flavor a rhythm to unfold. Let's orchestrate a symphony of learning, guided by time and temperature.\"\n- This function crafts a scheduler, a tool that meticulously controls the model's learning rate over time, ensuring optimal growth and preventing flavors from becoming overcooked or underdeveloped.\n\nHere's how it curates the learning journey:\n\n- Understanding the Recipe: It carefully considers the total number of training epochs (complete cycles through the training data), like a pastry chef planning how long to bake a complex cake.\n- Warming Up the Oven: It establishes a gradual learning rate increase at the beginning, allowing flavors to mingle harmoniously before full heat is applied, like preheating an oven to ensure even baking.\n- Regulating the Pace: It sets a linear decrease in learning rate over time, ensuring steady progress without rushing the model's development, like carefully lowering oven temperature to prevent overbrowning.\n- Accounting for Accumulation: It adjusts for gradient accumulation, a technique that blends multiple learning steps for smoother flavor development, like a pastry chef gently folding ingredients to create a light and airy batter.\n\nThe result?\n\n- Graceful Learning and Consistent Growth: With ```Create_Scheduler()```, our language model embarks on a carefully choreographed learning journey, guided by a schedule that fosters the gradual unfolding of flavors and prevents any aspect of language understanding from becoming burnt or undercooked.\n- A Symphony of Flavors in Perfect Harmony: This function ensures that each nuance of language is given ample time to integrate with the others, resulting in a language model that is not only capable of impressive feats but also exhibits a balance and refinement that is a delight to experience.","metadata":{}},{"cell_type":"code","source":"def Create_Scheduler(\n        optimizer,\n        train_dataloader,\n        num_warmup_steps,\n        num_train_epochs,\n        gradient_accumulation_steps\n    ):\n    \"\"\"\n    Creates a linear learning rate scheduler with warmup, adjusting for gradient accumulation.\n\n    Args:\n    - optimizer: The optimizer to create the scheduler for.\n    - train_dataloader: The training dataloader to calculate the total number of training steps.\n    - num_warmup_steps (int): The number of warmup steps for the scheduler.\n    - num_train_epochs (int): The total number of training epochs.\n    - gradient_accumulation_steps (int): The number of gradient accumulation steps.\n\n    Returns:\n    - transformers.get_linear_schedule_with_warmup: The created scheduler.\n\n    Raises:\n    - Exception: Raises any exceptions that occur during scheduler creation.\n    \"\"\"\n    try:\n        return get_linear_schedule_with_warmup(\n                    optimizer=optimizer,\n                    num_warmup_steps=num_warmup_steps,\n                    num_training_steps=(len(train_dataloader) * num_train_epochs) // gradient_accumulation_steps,\n                )\n    except Exception as e:\n        raise(e)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.117323Z","iopub.execute_input":"2023-10-28T18:27:05.117592Z","iopub.status.idle":"2023-10-28T18:27:05.132237Z","shell.execute_reply.started":"2023-10-28T18:27:05.117569Z","shell.execute_reply":"2023-10-28T18:27:05.131355Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### From Apprentice to Master: The Complete Training Journey","metadata":{}},{"cell_type":"markdown","source":"Finally, let's meet the head chef, the maestro of the grand kitchen: ```training_pipeline()```\n\nImagine this as the seasoned head chef who masterfully oversees the entire culinary process, ensuring that every ingredient, technique, and timing is flawlessly orchestrated to create a masterpiece of flavor and efficiency.\n\n- \"From pantry to plate, every step must be carefully choreographed. Let's bring this symphony of flavors to life, ensuring a feast for both the mind and the senses.\"\n- This function coordinates the entire training process, guiding the model's journey from raw ingredients to a fully realized, exquisitely crafted language master.\n\nHere's how it conducts the grand ballet:\n\n- Preparing the Kitchen: It gathers essential ingredients like the tokenizer (language segmentation), datasets (the nourishing texts for learning), and configurations (the recipes and techniques).\n- Setting the Table: It arranges the workspace, configuring the accelerator (like specialized cooking equipment) to optimize resource utilization and setting the seeds for reproducibility (ensuring consistent results, like measuring ingredients precisely).\n- Creating the Dish: It constructs the model with meticulous attention to detail, incorporating techniques like quantization (concentrating flavors for efficiency) and PEFT (enhancing model structure for optimal performance).\n- Adding the Spices: It carefully crafts an optimizer and scheduler (the seasonings that guide the learning process), ensuring a balanced and flavorful development of language understanding.\n- Hiring the Sous Chef: It appoints a trainer (the dedicated teacher who oversees the model's day-to-day learning), equipped with all the necessary tools and ingredients for effective instruction.\n- Inviting the Food Critic: It initializes WandB (the discerning observer who tracks progress and provides feedback), ensuring transparency and continuous refinement of the culinary journey.\n- Cooking with Precision: It monitors GPU utilization (like checking oven temperatures) to maintain a harmonious balance between flavor and resource efficiency.\n- The Grand Transformation: It ignites the learning process with trainer.train(), initiating the model's journey of transformation from a novice to a seasoned language master.\n- Recording the Journey: It meticulously logs metrics (like documenting the evolution of flavors) and saves checkpoints (like preserving signature dishes for future reference).\n- Sharing the Secrets: It optionally shares the model's knowledge with the world through the Hugging Face Hub (like publishing a cookbook for others to learn from).\n- Preserving the Legacy: It saves a final checkpoint (like storing a signature dish in the freezer for future enjoyment), ensuring the model's knowledge and expertise can be savored again and again.\n- Cleaning the Kitchen: It diligently cleans up the workspace, ensuring a smooth transition to future culinary adventures (like properly storing ingredients and washing dishes).\n\nThe result?\n\n- A Symphony of Flavors and Efficiency: With ```training_pipeline()```, language model training becomes a harmonious dance of knowledge, precision, and resourcefulness. It's a testament to the power of careful orchestration and attention to detail in creating experiences that not only delight the senses but also respect the constraints of the kitchen.\n- A Masterful Blend of Art and Science: This function embodies the delicate balance between creativity and precision, showcasing that even in the realm of language models, the most extraordinary creations often arise from a meticulous attention to detail and a deep understanding of the ingredients at play.","metadata":{}},{"cell_type":"code","source":"def training_pipeline(\n        model_config,\n        Config_BNB,\n        Config_LORA,\n        Config_wandb,\n        Config_Dataset,\n        Config_General,\n        training_config,\n        Config_Tokenizer,\n        Config_Accelerator\n    ):\n    \"\"\"\n    Orchestrates the training process of a quantized language model with PEFT and memory optimizations.\n\n    Args:\n    - model_config (dict): Configuration for the model architecture.\n    - Config_BNB (dict): Configuration for BitsAndBytes quantization.\n    - Config_LORA (dict): Configuration for PEFT (Lora).\n    - Config_wandb (dict): Configuration for Weights & Biases logging.\n    - Config_Dataset (dict): Configuration for dataset loading.\n    - Config_General (dict): General training configurations.\n    - training_config (dict): Training-specific configurations.\n    - Config_Tokenizer (dict): Configuration for the tokenizer.\n    - Config_Accelerator (dict): Configuration for the accelerator (e.g., DeepSpeed).\n    \"\"\"\n\n    # GPU Configurations\n    with torch.no_grad():\n        torch.cuda.empty_cache()\n    \n    print('\\n')\n    \n    print_gpu_utilization()\n    print('\\n')\n\n    !nvidia-smi\n    print('\\n')\n\n    # Fix seeds\n    fix_all_seeds(Config_General.SEED)\n    \n    # Fetch Tokenizer\n    tokenizer = get_tokenizer(MODEL_ID, Config_Tokenizer)\n    \n    # Fetch Datasets to train\n    train_dataset, valid_dataset = fetch_datasets(Config_Dataset)\n\n    # Fetch training configurations\n    train_dataloader = data_loader(\n                            train_dataset,\n                            batch_size = Config_General.BATCH_SIZE,\n                            pin_memory = Config_General.PIN_MEMORY,\n                            num_workers = Config_General.NUM_WORKERS,\n                            collate_fn = DataCollatorForCompletionOnlyLM(\n                                            tokenizer=tokenizer, \n                                            response_template=\"### Answer:\"\n                                        )\n                        )\n\n    # Set-up the accelerator\n    accelerator = Accelerator(\n                        split_batches = Config_Accelerator.SPLIT_BATCHES,\n                        mixed_precision = Config_Accelerator.MIXED_PRECISION,\n                        dispatch_batches = Config_Accelerator.DISPATCH_BATCHES,\n                        gradient_accumulation_steps = training_config.gradient_accumulation_steps,\n                    )\n\n    DEVICE=accelerator.device\n    N_GPUS = torch.cuda.device_count()\n    MAX_MEMORY = Config_General.MAX_MEMORY\n    process_index = accelerator.process_index\n\n    # BitsAndBytesConfig\n    BNB_Config = config_bnb(Config_BNB)\n\n    # LoraConfig \n    LORA_config = config_lora(Config_LORA)\n\n    # Create Model\n    model = make_model(\n                N_GPUS,\n                DEVICE,\n                MODEL_ID,\n                tokenizer,\n                MAX_MEMORY,\n                BNB_Config,\n                LORA_config,\n                process_index,\n                Config_General.SEQUENCE_MAX_LENGTH,\n                Config_BNB.BNB_4BIT_COMPUTE_DTYPE\n            )\n\n    # Print Model Info\n    accelerator.print(model.print_trainable_parameters())\n    \n    # Creating Optimizer\n    optimizer_custom = Create_Optimizer(\n                            model,\n                            optim_bits = Config_BNB.LOAD_BIT,\n                            adam_beta1 = training_config.adam_beta1,\n                            adam_beta2 = training_config.adam_beta2,\n                            adam_epsilon = training_config.adam_epsilon,\n                            weight_decay = training_config.weight_decay,\n                            learning_rate = training_config.learning_rate\n                        )\n\n    # Creating LR Schedular\n    LR_Scheduler = Create_Scheduler(\n                        optimizer = optimizer_custom,\n                        train_dataloader = train_dataloader,\n                        num_warmup_steps = training_config.warmup_steps,\n                        num_train_epochs = training_config.num_train_epochs,\n                        gradient_accumulation_steps = training_config.gradient_accumulation_steps\n                    )\n\n    # Set-up Trainer\n    trainer = SFTTrainer(\n                    model = model,\n                    tokenizer = tokenizer,\n                    args = training_config,\n                    peft_config = LORA_config,\n                    eval_dataset = valid_dataset,\n                    train_dataset = train_dataset,\n                    dataset_text_field = \"final_prompts\",\n                    optimizers = (optimizer_custom,LR_Scheduler),\n                    max_seq_length = Config_General.SEQUENCE_MAX_LENGTH,\n                    data_collator = DataCollatorForCompletionOnlyLM(\n                                        tokenizer=tokenizer, \n                                        response_template=\"### Answer:\"\n                                    )\n                )\n    \n    # Inititalizing WandB\n    wandb.init(\n        name=Config_wandb.NAME,\n        tags=Config_wandb.TAGS,\n        project=Config_wandb.PROJECT,\n        save_code=Config_wandb.SAVE_CODE,\n    )\n\n    # Check Memory space occupied\n    print_gpu_utilization()\n\n    train_result = trainer.train()\n    metrics = train_result.metrics\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n    print(metrics)\n\n    trainer.push_to_hub()\n\n\n    print(\"Saving last checkpoint of the model...\")\n    os.makedirs(training_config.output_dir, exist_ok=True)\n\n    if hasattr(trainer.model, 'module'):\n        model_to_save = trainer.model.module\n    else:\n        model_to_save = trainer.model  \n\n    model_to_save.save_pretrained(training_config.output_dir)\n\n    del model\n    del trainer\n    del tokenizer\n    del BNB_Config\n    del LORA_config\n    del LR_Scheduler\n    del train_dataset\n    del valid_dataset\n    del optimizer_custom\n    del training_config\n    gc.collect()\n\n    torch.cuda.empty_cache()\n\n    wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.133770Z","iopub.execute_input":"2023-10-28T18:27:05.134108Z","iopub.status.idle":"2023-10-28T18:27:05.162259Z","shell.execute_reply.started":"2023-10-28T18:27:05.134078Z","shell.execute_reply":"2023-10-28T18:27:05.161298Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Configuration, Assemble! ","metadata":{}},{"cell_type":"code","source":"# PEFT_MODEL_ID = '/kaggle/input/kaggle-scienc-llm-model/kaggle/working/Kaggle-Science-LLM'\nMODEL_ID = 'Adi-ds/Kaggle-Science-LLM'\nHUB_MODEL_ID = 'Kaggle-Science-LLM'\n\n@dataclass(frozen=True)\nclass Config_wandb:\n    SAVE_CODE: bool=True\n    NAME: str='training-phase-2'\n    TAGS=[\"experiment\",\"2-epoch\"]\n    PROJECT: str=\"Kaggle-LLM-Science\"\n\n@dataclass(frozen=True)\nclass Config_Dataset:\n    CREATE_ITERABLE: bool=False\n    TRAIN_CSV_PATH: str='/kaggle/input/llm-science-exam-final-data/final_training_data.csv'\n    VALID_CSV_PATH: str='/kaggle/input/llm-science-exam-final-data/final_validation_data.csv'\n\n@dataclass(frozen=True)\nclass Config_General:\n    SEED: int=69\n    BATCH_SIZE: int=2**2\n    NUM_WORKERS: int=2**1\n    PIN_MEMORY: bool=True\n    MAX_MEMORY: str=f'{15000}MB'\n    SEQUENCE_MAX_LENGTH: int=2**12\n\n@dataclass(frozen=True)\nclass Config_Accelerator:\n    SPLIT_BATCHES: bool=True\n    MIXED_PRECISION: str='fp16'\n    DISPATCH_BATCHES: bool=True\n        \n@dataclass(frozen=True)\nclass Config_LORA:\n    LORA_R: int=2**6\n    LORA_ALPHA: int=16\n    LORA_BIAS: str='none'\n    LORA_DROPOUT: float=0.1\n    TASK_TYPE: str=\"CAUSAL_LM\"\n\n@dataclass(frozen=True)\nclass Config_BNB:\n    LOAD_BIT: int=4\n    LOAD_IN_4BIT: bool=True\n    BNB_4BIT_QUANT_TYPE: str=\"nf4\"\n    BNB_4BIT_USE_DOUBLE_QUANT: bool=True\n    BNB_4BIT_COMPUTE_DTYPE=torch.bfloat16\n\n@dataclass(frozen=True)\nclass Config_Tokenizer:\n    PADDING: bool=True\n    TRUNCATION: bool=True\n    MODEL_NAME: str=MODEL_ID\n    USE_AUTH_TOKEN: bool=True\n    FORCE_DOWNLOAD: bool=True\n    RESUME_DOWNLOAD: bool=True\n    TRUST_REMOTE_CODE: bool=True\n    ADD_SPECIAL_TOKENS: bool=True\n\ntraining_config =  TrainingArguments(\n                        fp16 = True,\n                        max_steps=50,\n                        adam_beta1=0.9,\n                        adam_beta2=0.95,\n                        logging_steps=5,\n                        warmup_ratio = 0.03,\n                        weight_decay = 0.01,\n                        group_by_length=True,\n                        num_train_epochs = 3,\n                        learning_rate = 1.5e-5,\n                        auto_find_batch_size = True,\n                        lr_scheduler_type = 'cosine',\n                        label_smoothing_factor = 0.1,\n                        evaluation_strategy = \"steps\",\n                        gradient_checkpointing = True,\n                        gradient_accumulation_steps = 2**1,\n                        per_device_eval_batch_size = Config_General.BATCH_SIZE,\n                        per_device_train_batch_size = Config_General.BATCH_SIZE,\n                        sharded_ddp=False,\n                        push_to_hub = True,\n                        hub_private_repo = True,\n                        hub_model_id = HUB_MODEL_ID,\n                        overwrite_output_dir = True,\n                        load_best_model_at_end=True,\n                        remove_unused_columns = True,\n                        dataloader_pin_memory = Config_General.PIN_MEMORY,\n                        output_dir = os.getcwd()+'/Kaggle-Science-LLM',\n                        seed = Config_General.SEED,\n                        data_seed = Config_General.SEED\n                    )","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.163690Z","iopub.execute_input":"2023-10-28T18:27:05.164007Z","iopub.status.idle":"2023-10-28T18:27:05.187095Z","shell.execute_reply.started":"2023-10-28T18:27:05.163983Z","shell.execute_reply":"2023-10-28T18:27:05.186080Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Orchestrating Distributed Training : Let's Cook","metadata":{}},{"cell_type":"code","source":"notebook_launcher(\n    training_pipeline, \n    args=(\n        MODEL_ID,\n        Config_BNB,\n        Config_LORA,\n        Config_wandb,\n        Config_Dataset,\n        Config_General,\n        training_config,\n        Config_Tokenizer,\n        Config_Accelerator\n    ), \n    num_processes=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:27:05.188253Z","iopub.execute_input":"2023-10-28T18:27:05.188604Z","iopub.status.idle":"2023-10-28T22:02:16.660622Z","shell.execute_reply.started":"2023-10-28T18:27:05.188571Z","shell.execute_reply":"2023-10-28T22:02:16.659646Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Launching training on one GPU.\n\n\nGPU memory occupied: 2 MB.\n\n\nSat Oct 28 18:27:06 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/796 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c60b6c31c3741318b1ebe9d6bf0b6b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/796 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"721ddb8e20e54568aac65a388fb5c6b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7bc557aa2934467b23fa25b58a55ca5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221968ad2c8c4b3f9480ef3d9970de7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45ef446f2fc4ec495bc4ca6bb591b13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/796 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"842034d82c2e496eb1fba9c194b54df4"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-a6b90193c1ff733a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f1457ebcd24f51bccefd47829bef4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa2124bff0324d4e9cefee3b2c921bb0"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-a6b90193c1ff733a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"286ef72a47474b0da9f49d614ced9bfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/adapter_config.json:   0%|          | 0.00/446 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aa1a79f0b254b65a66164535d8b0fd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4c4a631b5747d092d4396cd930bcd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e6ac2b3f9a4d91ba81faac90693a74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8cb724c9d0e4b1abd93e468504e45cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"442ab13dd9c54020b1989a01bb99bc08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec4a6edf16546bb926ef86bd4452362"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992b756adb2a4d7ea81386658d591aa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46992e7f1db34721b55b91e6e7bc829b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading adapter_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005774055c96467bb0a7ac5aaf2b6692"}},"metadata":{}},{"name":"stdout","text":"trainable params: 33,554,432 || all params: 3,533,967,360 || trainable%: 0.9494833591219133\nNone\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7634232b0a04031a807e0927e07280c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2049bb3a71174f1188d1cd056816c7e9"}},"metadata":{}},{"name":"stderr","text":"Cloning https://huggingface.co/Adi-ds/Kaggle-Science-LLM into local empty directory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Download file adapter_model.bin:   0%|          | 15.4k/128M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b34cc6b615849dfaecd014615612d02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file runs/Oct10_08-12-26_0f24bac4bb12/events.out.tfevents.1696925895.0f24bac4bb12.70.0: 100%|########…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ddbefa37ff1408c9dd36b8a3a4f6672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file training_args.bin: 100%|##########| 3.87k/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3fc5e27180c420d9d32ac09c630c565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file tokenizer.model:   3%|3         | 15.4k/488k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e637b88edd84c75927a7ce7bb92cf65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file runs/Oct10_08-12-26_0f24bac4bb12/events.out.tfevents.1696925895.0f24bac4bb12.70.0:  11%|#1        |…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a68890cf6d40268a3aa35be76f2ef3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file training_args.bin:  26%|##5       | 1.00k/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84903bb1abde4b72b2b31e3669b7630f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file runs/Aug09_00-02-34_cb6b3aab7677/events.out.tfevents.1691539615.cb6b3aab7677.29.0: 100%|########…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5dbc83de53f4d24a07490ab8d64a1e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file runs/Aug09_00-02-34_cb6b3aab7677/events.out.tfevents.1691539615.cb6b3aab7677.29.0:  11%|#1        |…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b58c9681c18045169c0010d1106b4748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file tokenizer.model:   0%|          | 1.00k/488k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5af8d243fb6f49d2975ea3e1129d4a39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file adapter_model.bin:   0%|          | 1.00k/128M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04286532df864f37ae9393cbb144812b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.8"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231028_183513-s9m1tnpr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/adityadawn98/Kaggle-LLM-Science/runs/s9m1tnpr' target=\"_blank\">training-phase-2</a></strong> to <a href='https://wandb.ai/adityadawn98/Kaggle-LLM-Science' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/adityadawn98/Kaggle-LLM-Science' target=\"_blank\">https://wandb.ai/adityadawn98/Kaggle-LLM-Science</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/adityadawn98/Kaggle-LLM-Science/runs/s9m1tnpr' target=\"_blank\">https://wandb.ai/adityadawn98/Kaggle-LLM-Science/runs/s9m1tnpr</a>"},"metadata":{}},{"name":"stdout","text":"GPU memory occupied: 5989 MB.\n","output_type":"stream"},{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 3:25:24, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>6.667900</td>\n      <td>6.511322</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>6.484400</td>\n      <td>6.346075</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>6.252100</td>\n      <td>6.161617</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>6.088900</td>\n      <td>5.951450</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>5.829500</td>\n      <td>5.720163</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>5.607200</td>\n      <td>5.472379</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>5.339000</td>\n      <td>5.213606</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.098500</td>\n      <td>4.951412</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>4.887900</td>\n      <td>4.686074</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>4.631900</td>\n      <td>4.414491</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"***** train metrics *****\n  epoch                    =       0.08\n  total_flos               =  1660327GF\n  train_loss               =     5.6887\n  train_runtime            = 3:25:57.08\n  train_samples_per_second =      0.032\n  train_steps_per_second   =      0.004\n{'train_runtime': 12357.0887, 'train_samples_per_second': 0.032, 'train_steps_per_second': 0.004, 'total_flos': 1782763267522560.0, 'train_loss': 5.688729438781738, 'epoch': 0.08}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload file adapter_model.bin:   0%|          | 1.00/128M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0142a093e334bdcb62ce07ae4d2b30a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload file runs/Oct28_18-27-05_948ebb293dc9/events.out.tfevents.1698518147.948ebb293dc9.233.0:   0%|         …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"070c16b843284ff9bdd56c5425d8875c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload file training_args.bin:   0%|          | 1.00/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b2d755b673c4698bd5c59d9ac3b431d"}},"metadata":{}},{"name":"stderr","text":"To https://huggingface.co/Adi-ds/Kaggle-Science-LLM\n   ab64665..7e82808  main -> main\n\nTo https://huggingface.co/Adi-ds/Kaggle-Science-LLM\n   7e82808..b9548ec  main -> main\n\n","output_type":"stream"},{"name":"stdout","text":"Saving last checkpoint of the model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='7.012 MB of 7.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380ddf2561784114ac094bb0c2d4e2ad"}},"metadata":{}},{"name":"stderr","text":"wandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▃▂▁</td></tr><tr><td>eval/runtime</td><td>█▆█▃▆▅▅▁▄▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▃▁▆▃▄▄█▅▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▄▂▇▄▅▄█▅▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▄▃▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.41449</td></tr><tr><td>eval/runtime</td><td>1165.2141</td></tr><tr><td>eval/samples_per_second</td><td>1.437</td></tr><tr><td>eval/steps_per_second</td><td>0.36</td></tr><tr><td>train/epoch</td><td>0.08</td></tr><tr><td>train/global_step</td><td>50</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>4.6319</td></tr><tr><td>train/total_flos</td><td>1782763267522560.0</td></tr><tr><td>train/train_loss</td><td>5.68873</td></tr><tr><td>train/train_runtime</td><td>12357.0887</td></tr><tr><td>train/train_samples_per_second</td><td>0.032</td></tr><tr><td>train/train_steps_per_second</td><td>0.004</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">training-phase-2</strong> at: <a href='https://wandb.ai/adityadawn98/Kaggle-LLM-Science/runs/s9m1tnpr' target=\"_blank\">https://wandb.ai/adityadawn98/Kaggle-LLM-Science/runs/s9m1tnpr</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231028_183513-s9m1tnpr/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"Cooking done!\n\nThe next step is to serve the dish","metadata":{}},{"cell_type":"markdown","source":"# Please upvote if you like!","metadata":{}}]}